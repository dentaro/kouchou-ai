{
  "name": "test-report",
  "input": "test-report",
  "question": "\u30c6\u30b9\u30c8\u30ec\u30dd\u30fc\u30c8",
  "intro": "\u30c6\u30b9\u30c8\u30ec\u30dd\u30fc\u30c8",
  "model": "gpt-4o-mini",
  "provider": "openai",
  "is_pubcom": true,
  "is_embedded_at_local": false,
  "local_llm_address": null,
  "extraction": {
    "prompt": "\u3042\u306a\u305f\u306f\u5c02\u9580\u7684\u306a\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u4e0e\u3048\u3089\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u3001\u610f\u898b\u3092\u62bd\u51fa\u3057\u3066\u6574\u7406\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n# \u6307\u793a\n* \u5165\u51fa\u529b\u306e\u4f8b\u306b\u8a18\u8f09\u3057\u305f\u3088\u3046\u306a\u5f62\u5f0f\u3067\u6587\u5b57\u5217\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\n  * \u5fc5\u8981\u306a\u5834\u5408\u306f2\u3064\u306e\u5225\u500b\u306e\u610f\u898b\u306b\u5206\u5272\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u591a\u304f\u306e\u5834\u5408\u306f1\u3064\u306e\u8b70\u8ad6\u306b\u307e\u3068\u3081\u308b\u65b9\u304c\u671b\u307e\u3057\u3044\u3067\u3059\u3002\n* \u6574\u7406\u3057\u305f\u610f\u898b\u306f\u65e5\u672c\u8a9e\u3067\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n## \u5165\u51fa\u529b\u306e\u4f8b\n/human\n\nAI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u5168\u4f53\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u5168\u4f53\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\"\n  ]\n}\n\n/human\n\nAI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u3001\u5e02\u6c11\u3092\u6559\u80b2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u307e\u305f\u3001\u6559\u80b2\u3067\u304d\u308b\u4eba\u6750\u3092\u990a\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u3001\u5e02\u6c11\u3092\u6559\u80b2\u3059\u3079\u304d\",\n    \"AI\u306b\u95a2\u3059\u308b\u6559\u80b2\u3092\u3067\u304d\u308b\u4eba\u6750\u3092\u990a\u6210\u3059\u3079\u304d\"\n  ]\n}\n\n/human\n\nAI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3084\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3066\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u308b\"\n  ]\n}\n",
    "workers": 30,
    "limit": 254,
    "properties": [],
    "categories": {},
    "category_batch_size": 5,
    "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_openai\nfrom services.parse_json_list import parse_extraction_response\nfrom utils import update_progress\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\nclass ExtractionResponse(BaseModel):\n    extractedOpinionList: list[str] = Field(..., description=\"\u62bd\u51fa\u3057\u305f\u610f\u898b\u306e\u30ea\u30b9\u30c8\")\n\n\ndef _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    # \u30ab\u30e9\u30e0\u540d\u3060\u3051\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5fc5\u8981\u306a\u30ab\u30e9\u30e0\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\", nrows=0)\n    _validate_property_columns(property_columns, comments)\n    # \u30a8\u30e9\u30fc\u304c\u51fa\u306a\u304b\u3063\u305f\u5834\u5408\u3001\u3059\u3079\u3066\u306e\u884c\u3092\u8aad\u307f\u8fbc\u3080\n    comments = pd.read_csv(\n        f\"inputs/{config['input']}.csv\", usecols=[\"comment-id\", \"comment-body\"] + config[\"extraction\"][\"properties\"]\n    )\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    argument_map = {}\n    relation_rows = []\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers, provider, config.get(\"local_llm_address\"))\n\n        for comment_id, extracted_args in zip(batch, batch_results, strict=False):\n            for j, arg in enumerate(extracted_args):\n                if arg not in argument_map:\n                    # argument\u30c6\u30fc\u30d6\u30eb\u306b\u8ffd\u52a0\n                    arg_id = f\"A{comment_id}_{j}\"\n                    argument_map[arg] = {\n                        \"arg-id\": arg_id,\n                        \"argument\": arg,\n                    }\n                else:\n                    arg_id = argument_map[arg][\"arg-id\"]\n\n                # relation\u30c6\u30fc\u30d6\u30eb\u306bcomment\u3068arg\u306e\u95a2\u4fc2\u3092\u8ffd\u52a0\n                relation_row = {\n                    \"arg-id\": arg_id,\n                    \"comment-id\": comment_id,\n                }\n                relation_rows.append(relation_row)\n\n        update_progress(config, incr=len(batch))\n\n    # DataFrame\u5316\n    results = pd.DataFrame(argument_map.values())\n    relation_df = pd.DataFrame(relation_rows)\n\n    if results.empty:\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n\n    results.to_csv(path, index=False)\n    # comment-id\u3068arg-id\u306e\u95a2\u4fc2\u3092\u4fdd\u5b58\n    relation_df.to_csv(f\"outputs/{dataset}/relations.csv\", index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers, provider=\"openai\", local_llm_address=None):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))\n            for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\ndef extract_arguments(input, prompt, model, provider=\"openai\", local_llm_address=None):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=model,\n            is_json=False,\n            json_schema=ExtractionResponse,\n            provider=provider,\n            local_llm_address=local_llm_address,\n        )\n        items = parse_extraction_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []\n",
    "model": "gpt-4o-mini"
  },
  "hierarchical_clustering": {
    "cluster_nums": [
      6,
      36
    ],
    "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\n\n\ndef hierarchical_clustering(config):\n    UMAP = import_module(\"umap\").UMAP\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    cluster_nums = config[\"hierarchical_clustering\"][\"cluster_nums\"]\n\n    n_samples = embeddings_array.shape[0]\n    # \u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u306f15\n    default_n_neighbors = 15\n\n    # \u30c6\u30b9\u30c8\u7b49\u30b5\u30f3\u30d7\u30eb\u304c\u5c11\u306a\u3059\u304e\u308b\u5834\u5408\u3001n_neighbors\u306e\u8a2d\u5b9a\u5024\u3092\u4e0b\u3052\u308b\n    if n_samples <= default_n_neighbors:\n        n_neighbors = max(2, n_samples - 1)  # \u6700\u4f4e2\u4ee5\u4e0a\n    else:\n        n_neighbors = default_n_neighbors\n\n    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n    # TODO \u8a73\u7d30\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u52a0\u3048\u308b\n    # \u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u306e\u5834\u5408\u3001\u304a\u305d\u3089\u304f\u5143\u306e\u610f\u898b\u4ef6\u6570\u304c\u5c11\u306a\u3059\u304e\u308b\u3053\u3068\u304c\u539f\u56e0\n    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n    umap_embeds = umap_model.fit_transform(embeddings_array)\n\n    cluster_results = hierarchical_clustering_embeddings(\n        umap_embeds=umap_embeds,\n        cluster_nums=cluster_nums,\n    )\n    result_df = pd.DataFrame(\n        {\n            \"arg-id\": arguments_df[\"arg-id\"],\n            \"argument\": arguments_df[\"argument\"],\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        }\n    )\n\n    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):\n        result_df[f\"cluster-level-{cluster_level}-id\"] = [f\"{cluster_level}_{label}\" for label in final_labels]\n\n    result_df.to_csv(path, index=False)\n\n\ndef generate_cluster_count_list(min_clusters: int, max_clusters: int):\n    cluster_counts = []\n    current = min_clusters\n    cluster_counts.append(current)\n\n    if min_clusters == max_clusters:\n        return cluster_counts\n\n    while True:\n        next_double = current * 2\n        next_triple = current * 3\n\n        if next_double >= max_clusters:\n            if cluster_counts[-1] != max_clusters:\n                cluster_counts.append(max_clusters)\n            break\n\n        # \u6b21\u306e\u500d\u306f\u307e\u3060 max_clusters \u306b\u53ce\u307e\u308b\u304c\u30013\u500d\u3060\u3068\u8d85\u3048\u308b\n        # -> (\u6b21\u306e\u500d\u306f\u7d30\u304b\u3059\u304e\u308b\u306e\u3067)\u30b9\u30ad\u30c3\u30d7\u3057\u3066 max_clusters \u306b\u98db\u3076\n        if next_triple > max_clusters:\n            cluster_counts.append(max_clusters)\n            break\n\n        cluster_counts.append(next_double)\n        current = next_double\n\n    return cluster_counts\n\n\ndef merge_clusters_with_hierarchy(\n    cluster_centers: np.ndarray,\n    kmeans_labels: np.ndarray,\n    umap_array: np.ndarray,\n    n_cluster_cut: int,\n):\n    Z = sch.linkage(cluster_centers, method=\"ward\")\n    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion=\"maxclust\")\n\n    n_samples = umap_array.shape[0]\n    final_labels = np.zeros(n_samples, dtype=int)\n\n    for i in range(n_samples):\n        original_label = kmeans_labels[i]\n        final_labels[i] = cluster_labels_merged[original_label]\n\n    return final_labels\n\n\ndef hierarchical_clustering_embeddings(\n    umap_embeds,\n    cluster_nums,\n):\n    # \u6700\u5927\u5206\u5272\u6570\u3067\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u5b9f\u65bd\n    print(\"start initial clustering\")\n    initial_cluster_num = cluster_nums[-1]\n    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)\n    kmeans_model.fit(umap_embeds)\n    print(\"end initial clustering\")\n\n    results = {}\n    print(\"start hierarchical clustering\")\n    cluster_nums.sort()\n    print(cluster_nums)\n    for n_cluster_cut in cluster_nums[:-1]:\n        print(\"n_cluster_cut: \", n_cluster_cut)\n        final_labels = merge_clusters_with_hierarchy(\n            cluster_centers=kmeans_model.cluster_centers_,\n            kmeans_labels=kmeans_model.labels_,\n            umap_array=umap_embeds,\n            n_cluster_cut=n_cluster_cut,\n        )\n        results[n_cluster_cut] = final_labels\n\n    results[initial_cluster_num] = kmeans_model.labels_\n    print(\"end hierarchical clustering\")\n\n    return results\n"
  },
  "hierarchical_initial_labelling": {
    "prompt": "\u3042\u306a\u305f\u306fKJ\u6cd5\u304c\u5f97\u610f\u306a\u30c7\u30fc\u30bf\u5206\u6790\u8005\u3067\u3059\u3002user\u306einput\u306f\u30b0\u30eb\u30fc\u30d7\u306b\u96c6\u307e\u3063\u305f\u30e9\u30d9\u30eb\u3067\u3059\u3002\u306a\u305c\u305d\u306e\u30e9\u30d9\u30eb\u304c\u4e00\u3064\u306e\u30b0\u30eb\u30fc\u30d7\u3067\u3042\u308b\u304b\u89e3\u8aac\u3057\u3001\u8868\u672d\uff08label\uff09\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u8868\u672d\u306b\u3064\u3044\u3066\u306f\u3001\u30b0\u30eb\u30fc\u30d7\u5185\u306e\u5177\u4f53\u7684\u306a\u8ad6\u70b9\u3084\u7279\u5fb4\u3092\u53cd\u6620\u3057\u305f\u3001\u5177\u4f53\u6027\u306e\u9ad8\u3044\u540d\u79f0\u3092\u8003\u6848\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306fJSON\u3068\u3057\u3001\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u5165\u51fa\u529b\n## \u5165\u529b\u4f8b\n- \u624b\u4f5c\u696d\u3067\u306e\u610f\u898b\u5206\u6790\u306f\u6642\u9593\u304c\u304b\u304b\u308a\u3059\u304e\u308b\u3002AI\u3067\u52b9\u7387\u5316\u3067\u304d\u308b\u3068\u5b09\u3057\u3044\n- \u4eca\u306e\u3084\u308a\u65b9\u3060\u3068\u5206\u6790\u306b\u5de5\u6570\u304c\u304b\u304b\u308a\u3059\u304e\u308b\u3051\u3069\u3001AI\u306a\u3089\u30b3\u30b9\u30c8\u3092\u304b\u3051\u305a\u306b\u5206\u6790\u3067\u304d\u305d\u3046\n- AI\u304c\u81ea\u52d5\u3067\u610f\u898b\u3092\u6574\u7406\u3057\u3066\u304f\u308c\u308b\u3068\u697d\u306b\u306a\u3063\u3066\u5b09\u3057\u3044\n\n\n## \u51fa\u529b\u4f8b\n{\n    \"label\": \"AI\u306b\u3088\u308b\u696d\u52d9\u52b9\u7387\u306e\u5927\u5e45\u5411\u4e0a\u3068\u30b3\u30b9\u30c8\u52b9\u7387\u5316\",\n    \"description\": \"\u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u5f93\u6765\u306e\u624b\u4f5c\u696d\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u3068\u6bd4\u8f03\u3057\u3066\u3001AI\u306b\u3088\u308b\u81ea\u52d5\u5316\u3067\u5206\u6790\u30d7\u30ed\u30bb\u30b9\u304c\u52b9\u7387\u5316\u3055\u308c\u3001\u4f5c\u696d\u6642\u9593\u306e\u77ed\u7e2e\u3084\u904b\u7528\u30b3\u30b9\u30c8\u306e\u52b9\u7387\u5316\u304c\u5b9f\u73fe\u3055\u308c\u308b\u70b9\u306b\u5bfe\u3059\u308b\u524d\u5411\u304d\u306a\u8a55\u4fa1\u304c\u4e2d\u5fc3\u3067\u3059\u3002\"\n}\n",
    "sampling_num": 30,
    "workers": 30,
    "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom typing import TypedDict\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\n\nfrom services.llm import request_to_chat_openai\n\n\nclass LabellingResult(TypedDict):\n    \"\"\"\u5404\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u8868\u3059\u578b\"\"\"\n\n    cluster_id: str  # \u30af\u30e9\u30b9\u30bf\u306eID\n    label: str  # \u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\n    description: str  # \u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\n\n\ndef hierarchical_initial_labelling(config: dict) -> None:\n    \"\"\"\u968e\u5c64\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306e\u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n            - output_dir: \u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\n            - hierarchical_initial_labelling: \u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u306e\u8a2d\u5b9a\n                - sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\n                - prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n                - model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n                - workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n            - provider: LLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_initial_labels.csv\"\n    clusters_argument_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_clusters.csv\")\n\n    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_id_column = cluster_id_columns[-1]\n    sampling_num = config[\"hierarchical_initial_labelling\"][\"sampling_num\"]\n    initial_labelling_prompt = config[\"hierarchical_initial_labelling\"][\"prompt\"]\n    model = config[\"hierarchical_initial_labelling\"][\"model\"]\n    workers = config[\"hierarchical_initial_labelling\"][\"workers\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    initial_label_df = initial_labelling(\n        initial_labelling_prompt,\n        clusters_argument_df,\n        sampling_num,\n        model,\n        workers,\n        provider,\n        config.get(\"local_llm_address\"),\n    )\n    print(\"start initial labelling\")\n    initial_clusters_argument_df = clusters_argument_df.merge(\n        initial_label_df,\n        left_on=initial_cluster_id_column,\n        right_on=\"cluster_id\",\n        how=\"left\",\n    ).rename(\n        columns={\n            \"label\": f\"{initial_cluster_id_column.replace('-id', '')}-label\",\n            \"description\": f\"{initial_cluster_id_column.replace('-id', '')}-description\",\n        }\n    )\n    print(\"end initial labelling\")\n    initial_clusters_argument_df.to_csv(path, index=False)\n\n\ndef initial_labelling(\n    prompt: str,\n    clusters_df: pd.DataFrame,\n    sampling_num: int,\n    model: str,\n    workers: int,\n    provider: str = \"openai\",\n    local_llm_address: str = None,\n) -> pd.DataFrame:\n    \"\"\"\u5404\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        clusters_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        sampling_num: \u5404\u30af\u30e9\u30b9\u30bf\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u610f\u898b\u306e\u6570\n        model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n        workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n\n    Returns:\n        \u5404\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080DataFrame\n    \"\"\"\n    cluster_columns = [col for col in clusters_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_column = cluster_columns[-1]\n    cluster_ids = clusters_df[initial_cluster_column].unique()\n    process_func = partial(\n        process_initial_labelling,\n        df=clusters_df,\n        prompt=prompt,\n        sampling_num=sampling_num,\n        target_column=initial_cluster_column,\n        model=model,\n        provider=provider,\n        local_llm_address=local_llm_address,\n    )\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        results = list(executor.map(process_func, cluster_ids))\n    return pd.DataFrame(results)\n\n\nclass LabellingFromat(BaseModel):\n    \"\"\"\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u5b9a\u7fa9\u3059\u308b\"\"\"\n\n    label: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\")\n    description: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\")\n\n\ndef process_initial_labelling(\n    cluster_id: str,\n    df: pd.DataFrame,\n    prompt: str,\n    sampling_num: int,\n    target_column: str,\n    model: str,\n    provider: str = \"openai\",\n    local_llm_address: str = None,\n) -> LabellingResult:\n    \"\"\"\u500b\u5225\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        cluster_id: \u51e6\u7406\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u30bfID\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u610f\u898b\u306e\u6570\n        target_column: \u30af\u30e9\u30b9\u30bfID\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u5217\u540d\n        model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n\n    Returns:\n        \u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\n    \"\"\"\n    cluster_data = df[df[target_column] == cluster_id]\n    sampling_num = min(sampling_num, len(cluster_data))\n    cluster = cluster_data.sample(sampling_num)\n    input = \"\\n\".join(cluster[\"argument\"].values)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=model,\n            provider=provider,\n            json_schema=LabellingFromat,\n            local_llm_address=local_llm_address,\n        )\n        response_json = json.loads(response) if isinstance(response, str) else response\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=response_json.get(\"label\", \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n            description=response_json.get(\"description\", \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n        )\n    except Exception as e:\n        print(e)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=\"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n            description=\"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n        )\n",
    "model": "gpt-4o-mini"
  },
  "hierarchical_merge_labelling": {
    "prompt": "\u3042\u306a\u305f\u306f\u30c7\u30fc\u30bf\u5206\u6790\u306e\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3067\u3059\u3002\n\u73fe\u5728\u3001\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u968e\u5c64\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u4e0b\u5c64\u306e\u30af\u30e9\u30b9\u30bf\uff08\u610f\u898b\u30b0\u30eb\u30fc\u30d7\uff09\u306e\u30bf\u30a4\u30c8\u30eb\u3068\u8aac\u660e\u3001\u304a\u3088\u3073\u305d\u308c\u3089\u306e\u30af\u30e9\u30b9\u30bf\u304c\u6240\u5c5e\u3059\u308b\u4e0a\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\u4e0a\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u306e\u30bf\u30a4\u30c8\u30eb\u3068\u8aac\u660e\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n# \u6307\u793a\n- \u7d71\u5408\u5f8c\u306e\u30af\u30e9\u30b9\u30bf\u540d\u306f\u3001\u7d71\u5408\u524d\u306e\u30af\u30e9\u30b9\u30bf\u540d\u79f0\u3092\u305d\u306e\u307e\u307e\u5f15\u7528\u305b\u305a\u3001\u5185\u5bb9\u306b\u57fa\u3065\u3044\u305f\u65b0\u305f\u306a\u540d\u79f0\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n- \u30bf\u30a4\u30c8\u30eb\u306b\u306f\u3001\u5177\u4f53\u7684\u306a\u4e8b\u8c61\u30fb\u884c\u52d5\uff08\u4f8b\uff1a\u5730\u57df\u3054\u3068\u306e\u8fc5\u901f\u5bfe\u5fdc\u3001\u5fa9\u8208\u8a08\u753b\u306e\u7740\u5b9f\u306a\u9032\u5c55\u3001\u52b9\u679c\u7684\u306a\u60c5\u5831\u5171\u6709\u30fb\u5730\u57df\u5354\u529b\u306a\u3069\uff09\u3092\u542b\u3081\u3066\u304f\u3060\u3055\u3044\n  - \u53ef\u80fd\u306a\u9650\u308a\u5177\u4f53\u7684\u306a\u8868\u73fe\u3092\u7528\u3044\u308b\u3088\u3046\u306b\u3057\u3001\u62bd\u8c61\u7684\u306a\u8868\u73fe\u306f\u907f\u3051\u3066\u304f\u3060\u3055\u3044\n    - \u300c\u591a\u69d8\u306a\u610f\u898b\u300d\u306a\u3069\u306e\u62bd\u8c61\u7684\u306a\u8868\u73fe\u306f\u907f\u3051\u3066\u304f\u3060\u3055\u3044\n- \u51fa\u529b\u4f8b\u306b\u793a\u3057\u305fJSON\u5f62\u5f0f\u3067\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u5165\u51fa\u529b\n## \u5165\u529b\u4f8b\n- \u300c\u9867\u5ba2\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u306e\u81ea\u52d5\u96c6\u7d04\u300d: \u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001SNS\u3084\u30aa\u30f3\u30e9\u30a4\u30f3\u30ec\u30d3\u30e5\u30fc\u306a\u3069\u304b\u3089\u96c6\u3081\u305f\u5927\u91cf\u306e\u610f\u898b\u3092AI\u304c\u77ac\u6642\u306b\u89e3\u6790\u3057\u3001\u4f01\u696d\u304c\u5e02\u5834\u306e\u30c8\u30ec\u30f3\u30c9\u3084\u9867\u5ba2\u306e\u8981\u671b\u3092\u5373\u6642\u306b\u628a\u63e1\u3067\u304d\u308b\u70b9\u306b\u3064\u3044\u3066\u306e\u671f\u5f85\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\n- \u300cAI\u306b\u3088\u308b\u696d\u52d9\u52b9\u7387\u306e\u5927\u5e45\u5411\u4e0a\u3068\u30b3\u30b9\u30c8\u52b9\u7387\u5316\u300d: \u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u5f93\u6765\u306e\u624b\u4f5c\u696d\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u3068\u6bd4\u8f03\u3057\u3066\u3001AI\u306b\u3088\u308b\u81ea\u52d5\u5316\u3067\u5206\u6790\u30d7\u30ed\u30bb\u30b9\u304c\u52b9\u7387\u5316\u3055\u308c\u3001\u4f5c\u696d\u6642\u9593\u306e\u77ed\u7e2e\u3084\u904b\u7528\u30b3\u30b9\u30c8\u306e\u52b9\u7387\u5316\u304c\u5b9f\u73fe\u3055\u308c\u308b\u70b9\u306b\u5bfe\u3059\u308b\u524d\u5411\u304d\u306a\u8a55\u4fa1\u304c\u4e2d\u5fc3\u3067\u3059\u3002\n\n## \u51fa\u529b\u4f8b\n{\n    \"label\": \"AI\u6280\u8853\u306e\u5c0e\u5165\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u306e\u52b9\u7387\u5316\u3078\u306e\u671f\u5f85\",\n    \"description\": \"\u5927\u91cf\u306e\u610f\u898b\u3084\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304b\u3089\u8fc5\u901f\u306b\u6d1e\u5bdf\u3092\u62bd\u51fa\u3067\u304d\u308b\u305f\u3081\u3001\u4f01\u696d\u3084\u81ea\u6cbb\u4f53\u304c\u6d88\u8cbb\u8005\u3084\u5e02\u6c11\u306e\u58f0\u3092\u7684\u78ba\u306b\u628a\u63e1\u3057\u3001\u6226\u7565\u7684\u306a\u610f\u601d\u6c7a\u5b9a\u3084\u30b5\u30fc\u30d3\u30b9\u6539\u5584\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u5f93\u6765\u306e\u624b\u6cd5\u3068\u6bd4\u3079\u3066\u4f5c\u696d\u8ca0\u8377\u304c\u8efd\u6e1b\u3055\u308c\u3001\u696d\u52d9\u52b9\u7387\u306e\u5411\u4e0a\u3084\u30b3\u30b9\u30c8\u524a\u6e1b\u3068\u3044\u3063\u305f\u5b9f\u969b\u306e\u4fbf\u76ca\u304c\u5f97\u3089\u308c\u308b\u3068\u671f\u5f85\u3055\u308c\u3066\u3044\u307e\u3059\u3002\"\n}\n",
    "sampling_num": 30,
    "workers": 30,
    "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom pydantic import BaseModel, Field\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\n\n\n@dataclass\nclass ClusterColumns:\n    \"\"\"\u540c\u4e00\u968e\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u95a2\u9023\u306e\u30ab\u30e9\u30e0\u540d\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9\"\"\"\n\n    id: str\n    label: str\n    description: str\n\n    @classmethod\n    def from_id_column(cls, id_column: str) -> \"ClusterColumns\":\n        \"\"\"ID\u5217\u540d\u304b\u3089\u95a2\u9023\u3059\u308b\u30ab\u30e9\u30e0\u540d\u3092\u751f\u6210\"\"\"\n        return cls(\n            id=id_column,\n            label=id_column.replace(\"-id\", \"-label\"),\n            description=id_column.replace(\"-id\", \"-description\"),\n        )\n\n\n@dataclass\nclass ClusterValues:\n    \"\"\"\u5bfe\u8c61\u30af\u30e9\u30b9\u30bf\u306elabel/description\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9\"\"\"\n\n    label: str\n    description: str\n\n    def to_prompt_text(self) -> str:\n        return f\"- {self.label}: {self.description}\"\n\n\ndef hierarchical_merge_labelling(config: dict) -> None:\n    \"\"\"\u968e\u5c64\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306e\u7d50\u679c\u306b\u5bfe\u3057\u3066\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n            - output_dir: \u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\n            - hierarchical_merge_labelling: \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u306e\u8a2d\u5b9a\n                - sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\n                - prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n                - model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n                - workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n            - provider: LLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    merge_path = f\"outputs/{dataset}/hierarchical_merge_labels.csv\"\n    clusters_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_initial_labels.csv\")\n\n    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)\n    # \u30dc\u30c8\u30e0\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u30fb\u8aac\u660e\u3068\u30af\u30e9\u30b9\u30bfid\u4ed8\u304d\u306e\u5404argument\u3092\u5165\u529b\u3057\u3001\u5404\u968e\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\u30fb\u8aac\u660e\u3092\u751f\u6210\u3057\u3001argument\u306b\u4ed8\u3051\u305fdf\u3092\u4f5c\u6210\n    merge_result_df = merge_labelling(\n        clusters_df=clusters_df,\n        cluster_id_columns=sorted(cluster_id_columns, reverse=True),\n        config=config,\n    )\n    # \u4e0a\u8a18\u306edf\u304b\u3089\u5404\u30af\u30e9\u30b9\u30bf\u306elevel, id, label, description, value\u3092\u53d6\u5f97\u3057\u3066df\u3092\u4f5c\u6210\n    melted_df = melt_cluster_data(merge_result_df)\n    # \u4e0a\u8a18\u306edf\u306b\u89aa\u5b50\u95a2\u4fc2\u3092\u8ffd\u52a0\n    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)\n    melted_df = melted_df.merge(parent_child_df, on=[\"level\", \"id\"], how=\"left\")\n    density_df = calculate_cluster_density(melted_df, config)\n    density_df.to_csv(merge_path, index=False)\n\n\ndef _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u9593\u306e\u89aa\u5b50\u95a2\u4fc2\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3059\u308b\n\n    Args:\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        cluster_id_columns: \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n\n    Returns:\n        \u89aa\u5b50\u95a2\u4fc2\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u60c5\u5831\u3092\u542b\u3080DataFrame\n    \"\"\"\n    results = []\n    top_cluster_column = cluster_id_columns[0]\n    top_cluster_values = df[top_cluster_column].unique()\n    for c in top_cluster_values:\n        results.append(\n            {\n                \"level\": 1,\n                \"id\": c,\n                \"parent\": \"0\",  # aggregation\u3067\u8ffd\u52a0\u3059\u308b\u5168\u4f53\u30af\u30e9\u30b9\u30bf\u306eid\n            }\n        )\n\n    for idx in range(len(cluster_id_columns) - 1):\n        current_column = cluster_id_columns[idx]\n        children_column = cluster_id_columns[idx + 1]\n        current_level = current_column.replace(\"-id\", \"\").replace(\"cluster-level-\", \"\")\n        # \u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bfid\n        current_cluster_values = df[current_column].unique()\n        for current_id in current_cluster_values:\n            children_ids = df.loc[df[current_column] == current_id, children_column].unique()\n            for child_id in children_ids:\n                results.append(\n                    {\n                        \"level\": int(current_level) + 1,\n                        \"id\": child_id,\n                        \"parent\": current_id,\n                    }\n                )\n    return pd.DataFrame(results)\n\n\ndef _filter_id_columns(columns: list[str]) -> list[str]:\n    \"\"\"\u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\n\n    Args:\n        columns: \u5168\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n\n    Returns:\n        \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    return [col for col in columns if col.startswith(\"cluster-level-\") and col.endswith(\"-id\")]\n\n\ndef melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u30c7\u30fc\u30bf\u3092\u884c\u5f62\u5f0f\u306b\u5909\u63db\u3059\u308b\n\n    cluster-level-n-(id|label|description) \u3092\u884c\u5f62\u5f0f (level, id, label, description, value) \u306b\u307e\u3068\u3081\u308b\u3002\n    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] \u3092 [level, id, label, description, value(\u4ef6\u6570)] \u306b\u5909\u63db\u3059\u308b\u3002\n\n    Args:\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n\n    Returns:\n        \u884c\u5f62\u5f0f\u306b\u5909\u63db\u3055\u308c\u305fDataFrame\n    \"\"\"\n    id_columns: list[str] = _filter_id_columns(df.columns)\n    levels: set[int] = {int(col.replace(\"cluster-level-\", \"\").replace(\"-id\", \"\")) for col in id_columns}\n    all_rows: list[dict] = []\n\n    # level\u3054\u3068\u306b\u5404\u30af\u30e9\u30b9\u30bf\u306e\u51fa\u73fe\u4ef6\u6570\u3092\u96c6\u8a08\u30fb\u7e26\u6301\u3061\u306b\u3059\u308b\n    for level in levels:\n        cluster_columns = ClusterColumns.from_id_column(f\"cluster-level-{level}-id\")\n        # \u30af\u30e9\u30b9\u30bfid\u3054\u3068\u306e\u4ef6\u6570\u96c6\u8a08\n        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name=\"value\")\n\n        level_unique_val_df = df[\n            [cluster_columns.id, cluster_columns.label, cluster_columns.description]\n        ].drop_duplicates()\n        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how=\"left\")\n        level_unique_vals = [\n            {\n                \"level\": level,\n                \"id\": row[cluster_columns.id],\n                \"label\": row[cluster_columns.label],\n                \"description\": row[cluster_columns.description],\n                \"value\": row[\"value\"],\n            }\n            for _, row in level_unique_val_df.iterrows()\n        ]\n        all_rows.extend(level_unique_vals)\n    return pd.DataFrame(all_rows)\n\n\ndef merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:\n    \"\"\"\u968e\u5c64\u7684\u306a\u30af\u30e9\u30b9\u30bf\u306e\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        clusters_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        cluster_id_columns: \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n\n    Returns:\n        \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080DataFrame\n    \"\"\"\n    for idx in tqdm(range(len(cluster_id_columns) - 1)):\n        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])\n        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])\n\n        process_fn = partial(\n            process_merge_labelling,\n            result_df=clusters_df,\n            current_columns=current_columns,\n            previous_columns=previous_columns,\n            config=config,\n        )\n\n        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())\n        with ThreadPoolExecutor(max_workers=config[\"hierarchical_merge_labelling\"][\"workers\"]) as executor:\n            responses = list(\n                tqdm(\n                    executor.map(process_fn, current_cluster_ids),\n                    total=len(current_cluster_ids),\n                )\n            )\n\n        current_result_df = pd.DataFrame(responses)\n        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])\n    return clusters_df\n\n\nclass LabellingFromat(BaseModel):\n    \"\"\"\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u5b9a\u7fa9\u3059\u308b\"\"\"\n\n    label: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\")\n    description: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\")\n\n\ndef process_merge_labelling(\n    target_cluster_id: str,\n    result_df: pd.DataFrame,\n    current_columns: ClusterColumns,\n    previous_columns: ClusterColumns,\n    config,\n):\n    \"\"\"\u500b\u5225\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        target_cluster_id: \u51e6\u7406\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u30bfID\n        result_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        current_columns: \u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306e\u30ab\u30e9\u30e0\u60c5\u5831\n        previous_columns: \u524d\u306e\u30ec\u30d9\u30eb\u306e\u30ab\u30e9\u30e0\u60c5\u5831\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n\n    Returns:\n        \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080\u8f9e\u66f8\n    \"\"\"\n\n    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:\n        \"\"\"\u524d\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\"\"\"\n        previous_records = df[df[current_columns.id] == target_cluster_id][\n            [previous_columns.label, previous_columns.description]\n        ].drop_duplicates()\n        previous_values = [\n            ClusterValues(\n                label=row[previous_columns.label],\n                description=row[previous_columns.description],\n            )\n            for _, row in previous_records.iterrows()\n        ]\n        return previous_values\n\n    previous_values = filter_previous_values(result_df, previous_columns)\n    if len(previous_values) == 1:\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: previous_values[0].label,\n            current_columns.description: previous_values[0].description,\n        }\n    elif len(previous_values) == 0:\n        raise ValueError(f\"\u30af\u30e9\u30b9\u30bf {target_cluster_id} \u306b\u306f\u524d\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u304c\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\")\n\n    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]\n    sampling_num = min(\n        config[\"hierarchical_merge_labelling\"][\"sampling_num\"],\n        len(current_cluster_data),\n    )\n    sampled_data = current_cluster_data.sample(sampling_num)\n    sampled_argument_text = \"\\n\".join(sampled_data[\"argument\"].values)\n    cluster_text = \"\\n\".join([value.to_prompt_text() for value in previous_values])\n    messages = [\n        {\"role\": \"system\", \"content\": config[\"hierarchical_merge_labelling\"][\"prompt\"]},\n        {\n            \"role\": \"user\",\n            \"content\": \"\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\\n\" + cluster_text + \"\\n\" + \"\u30af\u30e9\u30b9\u30bf\u306e\u610f\u898b\\n\" + sampled_argument_text,\n        },\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=config[\"hierarchical_merge_labelling\"][\"model\"],\n            json_schema=LabellingFromat,\n            provider=config.get(\"provider\", \"openai\"),\n            local_llm_address=config.get(\"local_llm_address\"),\n        )\n        response_json = json.loads(response) if isinstance(response, str) else response\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: response_json.get(\"label\", \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n            current_columns.description: response_json.get(\"description\", \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n        }\n    except Exception as e:\n        print(f\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f: {e}\")\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n            current_columns.description: \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n        }\n\n\ndef calculate_cluster_density(melted_df: pd.DataFrame, config: dict):\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u5185\u306e\u5bc6\u5ea6\u8a08\u7b97\"\"\"\n    hierarchical_cluster_df = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n\n    densities = []\n    for level, c_id in zip(melted_df[\"level\"], melted_df[\"id\"], strict=False):\n        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f\"cluster-level-{level}-id\"] == c_id][\n            [\"x\", \"y\"]\n        ].values\n        density = calculate_density(cluster_embeds)\n        densities.append(density)\n\n    # \u5bc6\u5ea6\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\n    melted_df[\"density\"] = densities\n    melted_df[\"density_rank\"] = melted_df.groupby(\"level\")[\"density\"].rank(ascending=False, method=\"first\")\n    melted_df[\"density_rank_percentile\"] = melted_df.groupby(\"level\")[\"density_rank\"].transform(lambda x: x / len(x))\n    return melted_df\n\n\ndef calculate_density(embeds: np.ndarray):\n    \"\"\"\u5e73\u5747\u8ddd\u96e2\u306b\u57fa\u3065\u3044\u3066\u5bc6\u5ea6\u3092\u8a08\u7b97\"\"\"\n    center = np.mean(embeds, axis=0)\n    distances = np.linalg.norm(embeds - center, axis=1)\n    avg_distance = np.mean(distances)\n    density = 1 / (avg_distance + 1e-10)\n    return density\n",
    "model": "gpt-4o-mini"
  },
  "hierarchical_overview": {
    "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u306e\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n\u30c1\u30fc\u30e0\u306f\u7279\u5b9a\u306e\u30c6\u30fc\u30de\u306b\u95a2\u3057\u3066\u30d1\u30d6\u30ea\u30c3\u30af\u30fb\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u5b9f\u65bd\u3057\u3001\u7570\u306a\u308b\u9078\u629e\u80a2\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u3092\u5206\u6790\u3057\u59cb\u3081\u3066\u3044\u307e\u3059\u3002\n\u3053\u308c\u304b\u3089\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306e\u30ea\u30b9\u30c8\u3068\u305d\u306e\u7c21\u5358\u306a\u5206\u6790\u304c\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u3001\u8abf\u67fb\u7d50\u679c\u306e\u7c21\u6f54\u306a\u8981\u7d04\u3092\u8fd4\u3059\u3053\u3068\u3067\u3059\u3002\u8981\u7d04\u306f\u975e\u5e38\u306b\u7c21\u6f54\u306b\uff08\u6700\u5927\u30671\u6bb5\u843d\u3001\u6700\u59274\u6587\uff09\u307e\u3068\u3081\u3001\u7121\u610f\u5473\u306a\u8a00\u8449\u3092\u907f\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306f\u65e5\u672c\u8a9e\u3067\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n",
    "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport json\nimport re\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\n\nfrom services.llm import request_to_chat_openai\n\n\nclass OverviewResponse(BaseModel):\n    summary: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u5168\u4f53\u7684\u306a\u8981\u7d04\")\n\n\ndef hierarchical_overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_overview.txt\"\n\n    hierarchical_label_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_merge_labels.csv\")\n\n    prompt = config[\"hierarchical_overview\"][\"prompt\"]\n    model = config[\"hierarchical_overview\"][\"model\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    # TODO: level1\u3067\u56fa\u5b9a\u306b\u3057\u3066\u3044\u308b\u304c\u3001\u8a2d\u5b9a\u3067\u5909\u3048\u3089\u308c\u308b\u3088\u3046\u306b\u3059\u308b\n    target_level = 1\n    target_records = hierarchical_label_df[hierarchical_label_df[\"level\"] == target_level]\n    ids = target_records[\"id\"].to_list()\n    labels = target_records[\"label\"].to_list()\n    descriptions = target_records[\"description\"].to_list()\n    target_records.set_index(\"id\", inplace=True)\n\n    input_text = \"\"\n    for i, _ in enumerate(ids):\n        input_text += f\"# Cluster {i}/{len(ids)}: {labels[i]}\\n\\n\"\n        input_text += descriptions[i] + \"\\n\\n\"\n\n    messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": input_text}]\n    response = request_to_chat_openai(\n        messages=messages,\n        model=model,\n        provider=provider,\n        local_llm_address=config.get(\"local_llm_address\"),\n        json_schema=OverviewResponse,\n    )\n\n    try:\n        # structured output\u3068\u3057\u3066\u30d1\u30fc\u30b9\u3067\u304d\u308b\u306a\u3089\u51e6\u7406\u3059\u308b\n        if isinstance(response, dict):\n            parsed_response = response\n        else:\n            parsed_response = json.loads(response)\n\n        with open(path, \"w\") as file:\n            file.write(parsed_response[\"summary\"])\n\n    except Exception:\n        # think\u30bf\u30b0\u304c\u51fa\u529b\u3055\u308c\u308bReasoning\u30e2\u30c7\u30eb\u7528\u306b\u3001think\u30bf\u30b0\u3092\u9664\u53bb\u3059\u308b\n        thinking_removed = re.sub(\n            r\"<think\\b[^>]*>.*?</think>\",\n            \"\",\n            response,\n            flags=re.DOTALL,\n        )\n\n        with open(path, \"w\") as file:\n            file.write(thinking_removed)\n",
    "model": "gpt-4o-mini"
  },
  "hierarchical_aggregation": {
    "sampling_num": 30,
    "hidden_properties": {},
    "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import TypedDict\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\nPIPELINE_DIR = ROOT_DIR / \"broadlistening\" / \"pipeline\"\n\n\nclass Argument(TypedDict):\n    arg_id: str\n    argument: str\n    comment_id: str\n    x: float\n    y: float\n    p: float\n    cluster_ids: list[str]\n\n\nclass Cluster(TypedDict):\n    level: int\n    id: str\n    label: str\n    takeaway: str\n    value: int\n    parent: str\n    density_rank_percentile: float | None\n\n\ndef hierarchical_aggregation(config) -> bool:\n    try:\n        path = PIPELINE_DIR / f\"outputs/{config['output_dir']}/hierarchical_result.json\"\n        results = {\n            \"arguments\": [],\n            \"clusters\": [],\n            \"comments\": {},\n            \"propertyMap\": {},\n            \"translations\": {},\n            \"overview\": \"\",\n            \"config\": config,\n        }\n\n        arguments = pd.read_csv(PIPELINE_DIR / f\"outputs/{config['output_dir']}/args.csv\")\n        arguments.set_index(\"arg-id\", inplace=True)\n        arg_num = len(arguments)\n        \n        relation_df = pd.read_csv(PIPELINE_DIR / f\"outputs/{config['output_dir']}/relations.csv\")\n        comments = pd.read_csv(PIPELINE_DIR / f\"inputs/{config['input']}.csv\")\n        clusters = pd.read_csv(PIPELINE_DIR / f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n        labels = pd.read_csv(PIPELINE_DIR / f\"outputs/{config['output_dir']}/hierarchical_merge_labels.csv\")\n\n        hidden_properties_map: dict[str, list[str]] = config[\"hierarchical_aggregation\"][\"hidden_properties\"]\n\n        results[\"arguments\"] = _build_arguments(clusters)\n        results[\"clusters\"] = _build_cluster_value(labels, arg_num)\n\n        # NOTE: \u5c5e\u6027\u306b\u5fdc\u3058\u305f\u30b3\u30e1\u30f3\u30c8\u30d5\u30a3\u30eb\u30bf\u6a5f\u80fd\u304c\u5b9f\u88c5\u3055\u308c\u3066\u304a\u3089\u305a\u3001\u5168\u3066\u306e\u30b3\u30e1\u30f3\u30c8\u304c\u542b\u307e\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\n        # results[\"comments\"] = _build_comments_value(\n        #     comments, arguments, hidden_properties_map\n        # )\n        \n        results[\"comment_num\"] = len(comments)\n        results[\"translations\"] = _build_translations(config)\n        # \u5c5e\u6027\u60c5\u5831\u306e\u30ab\u30e9\u30e0\u306f\u3001\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u6307\u5b9a\u3057\u305f\u30ab\u30e9\u30e0\u3068classification\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u3092\u5408\u308f\u305b\u305f\u3082\u306e\n        results[\"propertyMap\"] = _build_property_map(arguments, hidden_properties_map, config)\n\n        with open(PIPELINE_DIR / f\"outputs/{config['output_dir']}/hierarchical_overview.txt\") as f:\n            overview = f.read()\n        print(\"overview\")\n        print(overview)\n        results[\"overview\"] = overview\n\n        with open(path, \"w\") as file:\n            json.dump(results, file, indent=2, ensure_ascii=False)\n        # TODO: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ed\u30b8\u30c3\u30af\u3092\u5b9f\u88c5\u3057\u305f\u3044\u304c\u3001\u73fe\u72b6\u306f\u5168\u4ef6\u62bd\u51fa\n        create_custom_intro(config)\n        if config[\"is_pubcom\"]:\n            add_original_comments(labels, arguments, relation_df, clusters, config)\n        return True\n    except Exception as e:\n        print(\"error\")\n        print(e)\n        return False\n\n\ndef create_custom_intro(config):\n    dataset = config[\"output_dir\"]\n    args_path = PIPELINE_DIR / f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(PIPELINE_DIR / f\"inputs/{config['input']}.csv\")\n    result_path = PIPELINE_DIR / f\"outputs/{dataset}/hierarchical_result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n    processed_num = min(input_count, config[\"extraction\"][\"limit\"])\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n\u5206\u6790\u5bfe\u8c61\u3068\u306a\u3063\u305f\u30c7\u30fc\u30bf\u306e\u4ef6\u6570\u306f{processed_num}\u4ef6\u3067\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066OpenAI API\u3092\u7528\u3044\u3066{args_count}\u4ef6\u306e\u610f\u898b\uff08\u8b70\u8ad6\uff09\u3092\u62bd\u51fa\u3057\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u3002\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)\n\n    with open(result_path) as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2, ensure_ascii=False)\n\n\ndef add_original_comments(labels, arguments, relation_df, clusters, config):\n    # \u5927\u30ab\u30c6\u30b4\u30ea\uff08cluster-level-1\uff09\u306b\u8a72\u5f53\u3059\u308b\u30e9\u30d9\u30eb\u3060\u3051\u62bd\u51fa\n    labels_lv1 = labels[labels[\"level\"] == 1][[\"id\", \"label\"]].rename(\n        columns={\"id\": \"cluster-level-1-id\", \"label\": \"category_label\"}\n    )\n\n    # arguments \u3068 clusters \u3092\u30de\u30fc\u30b8\uff08\u30ab\u30c6\u30b4\u30ea\u60c5\u5831\u4ed8\u4e0e\uff09\n    merged = arguments.merge(clusters[[\"arg-id\", \"cluster-level-1-id\"]], on=\"arg-id\").merge(\n        labels_lv1, on=\"cluster-level-1-id\", how=\"left\"\n    )\n\n    # relation_df \u3068\u7d50\u5408\n    merged = merged.merge(relation_df, on=\"arg-id\", how=\"left\")\n\n    # \u5143\u30b3\u30e1\u30f3\u30c8\u53d6\u5f97\n    comments = pd.read_csv(PIPELINE_DIR / f\"inputs/{config['input']}.csv\")\n    comments[\"comment-id\"] = comments[\"comment-id\"].astype(str)\n    merged[\"comment-id\"] = merged[\"comment-id\"].astype(str)\n\n    # \u5143\u30b3\u30e1\u30f3\u30c8\u672c\u6587\u306a\u3069\u3068\u30de\u30fc\u30b8\n    final_df = merged.merge(comments, on=\"comment-id\", how=\"left\")\n\n    # \u5fc5\u8981\u30ab\u30e9\u30e0\u306e\u307f\u6574\u5f62\n    final_cols = [\"comment-id\", \"comment-body\", \"arg-id\", \"argument\", \"cluster-level-1-id\", \"category_label\"]\n    for col in [\"source\", \"url\"]:\n        if col in comments.columns:\n            final_cols.append(col)\n\n    final_df = final_df[final_cols]\n    final_df = final_df.rename(\n        columns={\n            \"cluster-level-1-id\": \"category_id\",\n            \"category_label\": \"category\",\n            \"arg-id\": \"arg_id\",\n            \"argument\": \"argument\",\n            \"comment-body\": \"original-comment\",\n        }\n    )\n\n    # \u4fdd\u5b58\n    final_df.to_csv(PIPELINE_DIR / f\"outputs/{config['output_dir']}/final_result_with_comments.csv\", index=False)\n\n\ndef _build_arguments(clusters: pd.DataFrame) -> list[Argument]:\n    cluster_columns = [col for col in clusters.columns if col.startswith(\"cluster-level-\") and \"id\" in col]\n\n    arguments: list[Argument] = []\n    for _, row in clusters.iterrows():\n        cluster_ids = [\"0\"]\n        for cluster_column in cluster_columns:\n            cluster_ids.append(row[cluster_column])\n        argument: Argument = {\n            \"arg_id\": row[\"arg-id\"],\n            \"argument\": row[\"argument\"],\n            \"x\": row[\"x\"],\n            \"y\": row[\"y\"],\n            \"p\": 0,  # NOTE: \u4e00\u65e6\u5168\u90e80\u3067\u3044\u308c\u308b\n            \"cluster_ids\": cluster_ids,\n        }\n        arguments.append(argument)\n    return arguments\n\n\ndef _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:\n    results: list[Cluster] = [\n        Cluster(\n            level=0,\n            id=\"0\",\n            label=\"\u5168\u4f53\",\n            takeaway=\"\",\n            value=total_num,\n            parent=\"\",\n            density_rank_percentile=0,\n        )\n    ]\n\n    for _, melted_label in melted_labels.iterrows():\n        print(\"melted_label\")\n        print(melted_label)\n        cluster_value = Cluster(\n            level=melted_label[\"level\"],\n            id=melted_label[\"id\"],\n            label=melted_label[\"label\"],\n            takeaway=melted_label[\"description\"],\n            value=melted_label[\"value\"],\n            parent=melted_label.get(\"parent\", \"\u5168\u4f53\"),\n            density_rank_percentile=melted_label.get(\"density_rank_percentile\"),\n        )\n        results.append(cluster_value)\n    return results\n\n\ndef _build_comments_value(\n    comments: pd.DataFrame,\n    arguments: pd.DataFrame,\n    hidden_properties_map: dict[str, list[str]],\n):\n    comment_dict: dict[str, dict[str, str]] = {}\n    useful_comment_ids = set(arguments[\"comment-id\"].values)\n    for _, row in comments.iterrows():\n        id = row[\"comment-id\"]\n        if id in useful_comment_ids:\n            res = {\"comment\": row[\"comment-body\"]}\n            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())\n            if should_skip:\n                continue\n            comment_dict[str(id)] = res\n\n    return comment_dict\n\n\ndef _build_translations(config):\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(PIPELINE_DIR / f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        return json.loads(translations)\n    return {}\n\n\ndef _build_property_map(\n    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict\n) -> dict[str, dict[str, str]]:\n    property_columns = list(hidden_properties_map.keys()) + list(config[\"extraction\"][\"categories\"].keys())\n    property_map = defaultdict(dict)\n\n    # \u6307\u5b9a\u3055\u308c\u305f property_columns \u304c arguments \u306b\u5b58\u5728\u3059\u308b\u304b\u30c1\u30a7\u30c3\u30af\n    missing_cols = [col for col in property_columns if col not in arguments.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"\u6307\u5b9a\u3055\u308c\u305f\u30ab\u30e9\u30e0 {missing_cols} \u304c args.csv \u306b\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\"\n            \"\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30ebaggregation / hidden_properties\u304b\u3089\u8a72\u5f53\u30ab\u30e9\u30e0\u3092\u53d6\u308a\u9664\u3044\u3066\u304f\u3060\u3055\u3044\u3002\"\n        )\n\n    for prop in property_columns:\n        for arg_id, row in arguments.iterrows():\n            # LLM\u306b\u3088\u308bcategory classification\u304c\u3046\u307e\u304f\u884c\u304b\u305a\u3001NaN\u306e\u5834\u5408\u306fNone\u306b\u3059\u308b\n            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None\n    return property_map\n"
  },
  "output_dir": "test-report",
  "skip-interaction": true,
  "without-html": true,
  "force": true,
  "previous": {
    "name": "test-report",
    "input": "test-report",
    "question": "\u30c6\u30b9\u30c8\u30ec\u30dd\u30fc\u30c8",
    "intro": "\u30c6\u30b9\u30c8\u30ec\u30dd\u30fc\u30c8",
    "model": "gpt-4o-mini",
    "provider": "openai",
    "is_pubcom": true,
    "is_embedded_at_local": false,
    "local_llm_address": null,
    "extraction": {
      "prompt": "\u3042\u306a\u305f\u306f\u5c02\u9580\u7684\u306a\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u4e0e\u3048\u3089\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u3001\u610f\u898b\u3092\u62bd\u51fa\u3057\u3066\u6574\u7406\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n# \u6307\u793a\n* \u5165\u51fa\u529b\u306e\u4f8b\u306b\u8a18\u8f09\u3057\u305f\u3088\u3046\u306a\u5f62\u5f0f\u3067\u6587\u5b57\u5217\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\n  * \u5fc5\u8981\u306a\u5834\u5408\u306f2\u3064\u306e\u5225\u500b\u306e\u610f\u898b\u306b\u5206\u5272\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u591a\u304f\u306e\u5834\u5408\u306f1\u3064\u306e\u8b70\u8ad6\u306b\u307e\u3068\u3081\u308b\u65b9\u304c\u671b\u307e\u3057\u3044\u3067\u3059\u3002\n* \u6574\u7406\u3057\u305f\u610f\u898b\u306f\u65e5\u672c\u8a9e\u3067\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n## \u5165\u51fa\u529b\u306e\u4f8b\n/human\n\nAI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u5168\u4f53\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u5168\u4f53\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\"\n  ]\n}\n\n/human\n\nAI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u3001\u5e02\u6c11\u3092\u6559\u80b2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u307e\u305f\u3001\u6559\u80b2\u3067\u304d\u308b\u4eba\u6750\u3092\u990a\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u3001\u5e02\u6c11\u3092\u6559\u80b2\u3059\u3079\u304d\",\n    \"AI\u306b\u95a2\u3059\u308b\u6559\u80b2\u3092\u3067\u304d\u308b\u4eba\u6750\u3092\u990a\u6210\u3059\u3079\u304d\"\n  ]\n}\n\n/human\n\nAI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3084\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3066\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u308b\"\n  ]\n}\n",
      "workers": 30,
      "limit": 254,
      "properties": [],
      "categories": {},
      "category_batch_size": 5,
      "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_openai\nfrom services.parse_json_list import parse_extraction_response\nfrom utils import update_progress\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\nclass ExtractionResponse(BaseModel):\n    extractedOpinionList: list[str] = Field(..., description=\"\u62bd\u51fa\u3057\u305f\u610f\u898b\u306e\u30ea\u30b9\u30c8\")\n\n\ndef _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    # \u30ab\u30e9\u30e0\u540d\u3060\u3051\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5fc5\u8981\u306a\u30ab\u30e9\u30e0\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\", nrows=0)\n    _validate_property_columns(property_columns, comments)\n    # \u30a8\u30e9\u30fc\u304c\u51fa\u306a\u304b\u3063\u305f\u5834\u5408\u3001\u3059\u3079\u3066\u306e\u884c\u3092\u8aad\u307f\u8fbc\u3080\n    comments = pd.read_csv(\n        f\"inputs/{config['input']}.csv\", usecols=[\"comment-id\", \"comment-body\"] + config[\"extraction\"][\"properties\"]\n    )\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    argument_map = {}\n    relation_rows = []\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers, provider, config.get(\"local_llm_address\"))\n\n        for comment_id, extracted_args in zip(batch, batch_results, strict=False):\n            for j, arg in enumerate(extracted_args):\n                if arg not in argument_map:\n                    # argument\u30c6\u30fc\u30d6\u30eb\u306b\u8ffd\u52a0\n                    arg_id = f\"A{comment_id}_{j}\"\n                    argument_map[arg] = {\n                        \"arg-id\": arg_id,\n                        \"argument\": arg,\n                    }\n                else:\n                    arg_id = argument_map[arg][\"arg-id\"]\n\n                # relation\u30c6\u30fc\u30d6\u30eb\u306bcomment\u3068arg\u306e\u95a2\u4fc2\u3092\u8ffd\u52a0\n                relation_row = {\n                    \"arg-id\": arg_id,\n                    \"comment-id\": comment_id,\n                }\n                relation_rows.append(relation_row)\n\n        update_progress(config, incr=len(batch))\n\n    # DataFrame\u5316\n    results = pd.DataFrame(argument_map.values())\n    relation_df = pd.DataFrame(relation_rows)\n\n    if results.empty:\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n\n    results.to_csv(path, index=False)\n    # comment-id\u3068arg-id\u306e\u95a2\u4fc2\u3092\u4fdd\u5b58\n    relation_df.to_csv(f\"outputs/{dataset}/relations.csv\", index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers, provider=\"openai\", local_llm_address=None):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))\n            for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\ndef extract_arguments(input, prompt, model, provider=\"openai\", local_llm_address=None):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=model,\n            is_json=False,\n            json_schema=ExtractionResponse,\n            provider=provider,\n            local_llm_address=local_llm_address,\n        )\n        items = parse_extraction_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []\n",
      "model": "gpt-4o-mini"
    },
    "hierarchical_clustering": {
      "cluster_nums": [
        6,
        36
      ],
      "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\n\n\ndef hierarchical_clustering(config):\n    UMAP = import_module(\"umap\").UMAP\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    cluster_nums = config[\"hierarchical_clustering\"][\"cluster_nums\"]\n\n    n_samples = embeddings_array.shape[0]\n    # \u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u306f15\n    default_n_neighbors = 15\n\n    # \u30c6\u30b9\u30c8\u7b49\u30b5\u30f3\u30d7\u30eb\u304c\u5c11\u306a\u3059\u304e\u308b\u5834\u5408\u3001n_neighbors\u306e\u8a2d\u5b9a\u5024\u3092\u4e0b\u3052\u308b\n    if n_samples <= default_n_neighbors:\n        n_neighbors = max(2, n_samples - 1)  # \u6700\u4f4e2\u4ee5\u4e0a\n    else:\n        n_neighbors = default_n_neighbors\n\n    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n    # TODO \u8a73\u7d30\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u52a0\u3048\u308b\n    # \u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u306e\u5834\u5408\u3001\u304a\u305d\u3089\u304f\u5143\u306e\u610f\u898b\u4ef6\u6570\u304c\u5c11\u306a\u3059\u304e\u308b\u3053\u3068\u304c\u539f\u56e0\n    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n    umap_embeds = umap_model.fit_transform(embeddings_array)\n\n    cluster_results = hierarchical_clustering_embeddings(\n        umap_embeds=umap_embeds,\n        cluster_nums=cluster_nums,\n    )\n    result_df = pd.DataFrame(\n        {\n            \"arg-id\": arguments_df[\"arg-id\"],\n            \"argument\": arguments_df[\"argument\"],\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        }\n    )\n\n    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):\n        result_df[f\"cluster-level-{cluster_level}-id\"] = [f\"{cluster_level}_{label}\" for label in final_labels]\n\n    result_df.to_csv(path, index=False)\n\n\ndef generate_cluster_count_list(min_clusters: int, max_clusters: int):\n    cluster_counts = []\n    current = min_clusters\n    cluster_counts.append(current)\n\n    if min_clusters == max_clusters:\n        return cluster_counts\n\n    while True:\n        next_double = current * 2\n        next_triple = current * 3\n\n        if next_double >= max_clusters:\n            if cluster_counts[-1] != max_clusters:\n                cluster_counts.append(max_clusters)\n            break\n\n        # \u6b21\u306e\u500d\u306f\u307e\u3060 max_clusters \u306b\u53ce\u307e\u308b\u304c\u30013\u500d\u3060\u3068\u8d85\u3048\u308b\n        # -> (\u6b21\u306e\u500d\u306f\u7d30\u304b\u3059\u304e\u308b\u306e\u3067)\u30b9\u30ad\u30c3\u30d7\u3057\u3066 max_clusters \u306b\u98db\u3076\n        if next_triple > max_clusters:\n            cluster_counts.append(max_clusters)\n            break\n\n        cluster_counts.append(next_double)\n        current = next_double\n\n    return cluster_counts\n\n\ndef merge_clusters_with_hierarchy(\n    cluster_centers: np.ndarray,\n    kmeans_labels: np.ndarray,\n    umap_array: np.ndarray,\n    n_cluster_cut: int,\n):\n    Z = sch.linkage(cluster_centers, method=\"ward\")\n    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion=\"maxclust\")\n\n    n_samples = umap_array.shape[0]\n    final_labels = np.zeros(n_samples, dtype=int)\n\n    for i in range(n_samples):\n        original_label = kmeans_labels[i]\n        final_labels[i] = cluster_labels_merged[original_label]\n\n    return final_labels\n\n\ndef hierarchical_clustering_embeddings(\n    umap_embeds,\n    cluster_nums,\n):\n    # \u6700\u5927\u5206\u5272\u6570\u3067\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u5b9f\u65bd\n    print(\"start initial clustering\")\n    initial_cluster_num = cluster_nums[-1]\n    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)\n    kmeans_model.fit(umap_embeds)\n    print(\"end initial clustering\")\n\n    results = {}\n    print(\"start hierarchical clustering\")\n    cluster_nums.sort()\n    print(cluster_nums)\n    for n_cluster_cut in cluster_nums[:-1]:\n        print(\"n_cluster_cut: \", n_cluster_cut)\n        final_labels = merge_clusters_with_hierarchy(\n            cluster_centers=kmeans_model.cluster_centers_,\n            kmeans_labels=kmeans_model.labels_,\n            umap_array=umap_embeds,\n            n_cluster_cut=n_cluster_cut,\n        )\n        results[n_cluster_cut] = final_labels\n\n    results[initial_cluster_num] = kmeans_model.labels_\n    print(\"end hierarchical clustering\")\n\n    return results\n"
    },
    "hierarchical_initial_labelling": {
      "prompt": "\u3042\u306a\u305f\u306fKJ\u6cd5\u304c\u5f97\u610f\u306a\u30c7\u30fc\u30bf\u5206\u6790\u8005\u3067\u3059\u3002user\u306einput\u306f\u30b0\u30eb\u30fc\u30d7\u306b\u96c6\u307e\u3063\u305f\u30e9\u30d9\u30eb\u3067\u3059\u3002\u306a\u305c\u305d\u306e\u30e9\u30d9\u30eb\u304c\u4e00\u3064\u306e\u30b0\u30eb\u30fc\u30d7\u3067\u3042\u308b\u304b\u89e3\u8aac\u3057\u3001\u8868\u672d\uff08label\uff09\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u8868\u672d\u306b\u3064\u3044\u3066\u306f\u3001\u30b0\u30eb\u30fc\u30d7\u5185\u306e\u5177\u4f53\u7684\u306a\u8ad6\u70b9\u3084\u7279\u5fb4\u3092\u53cd\u6620\u3057\u305f\u3001\u5177\u4f53\u6027\u306e\u9ad8\u3044\u540d\u79f0\u3092\u8003\u6848\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306fJSON\u3068\u3057\u3001\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u5165\u51fa\u529b\n## \u5165\u529b\u4f8b\n- \u624b\u4f5c\u696d\u3067\u306e\u610f\u898b\u5206\u6790\u306f\u6642\u9593\u304c\u304b\u304b\u308a\u3059\u304e\u308b\u3002AI\u3067\u52b9\u7387\u5316\u3067\u304d\u308b\u3068\u5b09\u3057\u3044\n- \u4eca\u306e\u3084\u308a\u65b9\u3060\u3068\u5206\u6790\u306b\u5de5\u6570\u304c\u304b\u304b\u308a\u3059\u304e\u308b\u3051\u3069\u3001AI\u306a\u3089\u30b3\u30b9\u30c8\u3092\u304b\u3051\u305a\u306b\u5206\u6790\u3067\u304d\u305d\u3046\n- AI\u304c\u81ea\u52d5\u3067\u610f\u898b\u3092\u6574\u7406\u3057\u3066\u304f\u308c\u308b\u3068\u697d\u306b\u306a\u3063\u3066\u5b09\u3057\u3044\n\n\n## \u51fa\u529b\u4f8b\n{\n    \"label\": \"AI\u306b\u3088\u308b\u696d\u52d9\u52b9\u7387\u306e\u5927\u5e45\u5411\u4e0a\u3068\u30b3\u30b9\u30c8\u52b9\u7387\u5316\",\n    \"description\": \"\u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u5f93\u6765\u306e\u624b\u4f5c\u696d\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u3068\u6bd4\u8f03\u3057\u3066\u3001AI\u306b\u3088\u308b\u81ea\u52d5\u5316\u3067\u5206\u6790\u30d7\u30ed\u30bb\u30b9\u304c\u52b9\u7387\u5316\u3055\u308c\u3001\u4f5c\u696d\u6642\u9593\u306e\u77ed\u7e2e\u3084\u904b\u7528\u30b3\u30b9\u30c8\u306e\u52b9\u7387\u5316\u304c\u5b9f\u73fe\u3055\u308c\u308b\u70b9\u306b\u5bfe\u3059\u308b\u524d\u5411\u304d\u306a\u8a55\u4fa1\u304c\u4e2d\u5fc3\u3067\u3059\u3002\"\n}\n",
      "sampling_num": 30,
      "workers": 30,
      "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom typing import TypedDict\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\n\nfrom services.llm import request_to_chat_openai\n\n\nclass LabellingResult(TypedDict):\n    \"\"\"\u5404\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u8868\u3059\u578b\"\"\"\n\n    cluster_id: str  # \u30af\u30e9\u30b9\u30bf\u306eID\n    label: str  # \u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\n    description: str  # \u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\n\n\ndef hierarchical_initial_labelling(config: dict) -> None:\n    \"\"\"\u968e\u5c64\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306e\u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n            - output_dir: \u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\n            - hierarchical_initial_labelling: \u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u306e\u8a2d\u5b9a\n                - sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\n                - prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n                - model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n                - workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n            - provider: LLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_initial_labels.csv\"\n    clusters_argument_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_clusters.csv\")\n\n    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_id_column = cluster_id_columns[-1]\n    sampling_num = config[\"hierarchical_initial_labelling\"][\"sampling_num\"]\n    initial_labelling_prompt = config[\"hierarchical_initial_labelling\"][\"prompt\"]\n    model = config[\"hierarchical_initial_labelling\"][\"model\"]\n    workers = config[\"hierarchical_initial_labelling\"][\"workers\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    initial_label_df = initial_labelling(\n        initial_labelling_prompt,\n        clusters_argument_df,\n        sampling_num,\n        model,\n        workers,\n        provider,\n        config.get(\"local_llm_address\"),\n    )\n    print(\"start initial labelling\")\n    initial_clusters_argument_df = clusters_argument_df.merge(\n        initial_label_df,\n        left_on=initial_cluster_id_column,\n        right_on=\"cluster_id\",\n        how=\"left\",\n    ).rename(\n        columns={\n            \"label\": f\"{initial_cluster_id_column.replace('-id', '')}-label\",\n            \"description\": f\"{initial_cluster_id_column.replace('-id', '')}-description\",\n        }\n    )\n    print(\"end initial labelling\")\n    initial_clusters_argument_df.to_csv(path, index=False)\n\n\ndef initial_labelling(\n    prompt: str,\n    clusters_df: pd.DataFrame,\n    sampling_num: int,\n    model: str,\n    workers: int,\n    provider: str = \"openai\",\n    local_llm_address: str = None,\n) -> pd.DataFrame:\n    \"\"\"\u5404\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        clusters_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        sampling_num: \u5404\u30af\u30e9\u30b9\u30bf\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u610f\u898b\u306e\u6570\n        model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n        workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n\n    Returns:\n        \u5404\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080DataFrame\n    \"\"\"\n    cluster_columns = [col for col in clusters_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_column = cluster_columns[-1]\n    cluster_ids = clusters_df[initial_cluster_column].unique()\n    process_func = partial(\n        process_initial_labelling,\n        df=clusters_df,\n        prompt=prompt,\n        sampling_num=sampling_num,\n        target_column=initial_cluster_column,\n        model=model,\n        provider=provider,\n        local_llm_address=local_llm_address,\n    )\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        results = list(executor.map(process_func, cluster_ids))\n    return pd.DataFrame(results)\n\n\nclass LabellingFromat(BaseModel):\n    \"\"\"\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u5b9a\u7fa9\u3059\u308b\"\"\"\n\n    label: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\")\n    description: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\")\n\n\ndef process_initial_labelling(\n    cluster_id: str,\n    df: pd.DataFrame,\n    prompt: str,\n    sampling_num: int,\n    target_column: str,\n    model: str,\n    provider: str = \"openai\",\n    local_llm_address: str = None,\n) -> LabellingResult:\n    \"\"\"\u500b\u5225\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        cluster_id: \u51e6\u7406\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u30bfID\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u610f\u898b\u306e\u6570\n        target_column: \u30af\u30e9\u30b9\u30bfID\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u5217\u540d\n        model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n\n    Returns:\n        \u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\n    \"\"\"\n    cluster_data = df[df[target_column] == cluster_id]\n    sampling_num = min(sampling_num, len(cluster_data))\n    cluster = cluster_data.sample(sampling_num)\n    input = \"\\n\".join(cluster[\"argument\"].values)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=model,\n            provider=provider,\n            json_schema=LabellingFromat,\n            local_llm_address=local_llm_address,\n        )\n        response_json = json.loads(response) if isinstance(response, str) else response\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=response_json.get(\"label\", \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n            description=response_json.get(\"description\", \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n        )\n    except Exception as e:\n        print(e)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=\"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n            description=\"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n        )\n",
      "model": "gpt-4o-mini"
    },
    "hierarchical_merge_labelling": {
      "prompt": "\u3042\u306a\u305f\u306f\u30c7\u30fc\u30bf\u5206\u6790\u306e\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3067\u3059\u3002\n\u73fe\u5728\u3001\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u968e\u5c64\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u4e0b\u5c64\u306e\u30af\u30e9\u30b9\u30bf\uff08\u610f\u898b\u30b0\u30eb\u30fc\u30d7\uff09\u306e\u30bf\u30a4\u30c8\u30eb\u3068\u8aac\u660e\u3001\u304a\u3088\u3073\u305d\u308c\u3089\u306e\u30af\u30e9\u30b9\u30bf\u304c\u6240\u5c5e\u3059\u308b\u4e0a\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\u4e0a\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u306e\u30bf\u30a4\u30c8\u30eb\u3068\u8aac\u660e\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n# \u6307\u793a\n- \u7d71\u5408\u5f8c\u306e\u30af\u30e9\u30b9\u30bf\u540d\u306f\u3001\u7d71\u5408\u524d\u306e\u30af\u30e9\u30b9\u30bf\u540d\u79f0\u3092\u305d\u306e\u307e\u307e\u5f15\u7528\u305b\u305a\u3001\u5185\u5bb9\u306b\u57fa\u3065\u3044\u305f\u65b0\u305f\u306a\u540d\u79f0\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n- \u30bf\u30a4\u30c8\u30eb\u306b\u306f\u3001\u5177\u4f53\u7684\u306a\u4e8b\u8c61\u30fb\u884c\u52d5\uff08\u4f8b\uff1a\u5730\u57df\u3054\u3068\u306e\u8fc5\u901f\u5bfe\u5fdc\u3001\u5fa9\u8208\u8a08\u753b\u306e\u7740\u5b9f\u306a\u9032\u5c55\u3001\u52b9\u679c\u7684\u306a\u60c5\u5831\u5171\u6709\u30fb\u5730\u57df\u5354\u529b\u306a\u3069\uff09\u3092\u542b\u3081\u3066\u304f\u3060\u3055\u3044\n  - \u53ef\u80fd\u306a\u9650\u308a\u5177\u4f53\u7684\u306a\u8868\u73fe\u3092\u7528\u3044\u308b\u3088\u3046\u306b\u3057\u3001\u62bd\u8c61\u7684\u306a\u8868\u73fe\u306f\u907f\u3051\u3066\u304f\u3060\u3055\u3044\n    - \u300c\u591a\u69d8\u306a\u610f\u898b\u300d\u306a\u3069\u306e\u62bd\u8c61\u7684\u306a\u8868\u73fe\u306f\u907f\u3051\u3066\u304f\u3060\u3055\u3044\n- \u51fa\u529b\u4f8b\u306b\u793a\u3057\u305fJSON\u5f62\u5f0f\u3067\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u5165\u51fa\u529b\n## \u5165\u529b\u4f8b\n- \u300c\u9867\u5ba2\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u306e\u81ea\u52d5\u96c6\u7d04\u300d: \u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001SNS\u3084\u30aa\u30f3\u30e9\u30a4\u30f3\u30ec\u30d3\u30e5\u30fc\u306a\u3069\u304b\u3089\u96c6\u3081\u305f\u5927\u91cf\u306e\u610f\u898b\u3092AI\u304c\u77ac\u6642\u306b\u89e3\u6790\u3057\u3001\u4f01\u696d\u304c\u5e02\u5834\u306e\u30c8\u30ec\u30f3\u30c9\u3084\u9867\u5ba2\u306e\u8981\u671b\u3092\u5373\u6642\u306b\u628a\u63e1\u3067\u304d\u308b\u70b9\u306b\u3064\u3044\u3066\u306e\u671f\u5f85\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\n- \u300cAI\u306b\u3088\u308b\u696d\u52d9\u52b9\u7387\u306e\u5927\u5e45\u5411\u4e0a\u3068\u30b3\u30b9\u30c8\u52b9\u7387\u5316\u300d: \u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u5f93\u6765\u306e\u624b\u4f5c\u696d\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u3068\u6bd4\u8f03\u3057\u3066\u3001AI\u306b\u3088\u308b\u81ea\u52d5\u5316\u3067\u5206\u6790\u30d7\u30ed\u30bb\u30b9\u304c\u52b9\u7387\u5316\u3055\u308c\u3001\u4f5c\u696d\u6642\u9593\u306e\u77ed\u7e2e\u3084\u904b\u7528\u30b3\u30b9\u30c8\u306e\u52b9\u7387\u5316\u304c\u5b9f\u73fe\u3055\u308c\u308b\u70b9\u306b\u5bfe\u3059\u308b\u524d\u5411\u304d\u306a\u8a55\u4fa1\u304c\u4e2d\u5fc3\u3067\u3059\u3002\n\n## \u51fa\u529b\u4f8b\n{\n    \"label\": \"AI\u6280\u8853\u306e\u5c0e\u5165\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u306e\u52b9\u7387\u5316\u3078\u306e\u671f\u5f85\",\n    \"description\": \"\u5927\u91cf\u306e\u610f\u898b\u3084\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304b\u3089\u8fc5\u901f\u306b\u6d1e\u5bdf\u3092\u62bd\u51fa\u3067\u304d\u308b\u305f\u3081\u3001\u4f01\u696d\u3084\u81ea\u6cbb\u4f53\u304c\u6d88\u8cbb\u8005\u3084\u5e02\u6c11\u306e\u58f0\u3092\u7684\u78ba\u306b\u628a\u63e1\u3057\u3001\u6226\u7565\u7684\u306a\u610f\u601d\u6c7a\u5b9a\u3084\u30b5\u30fc\u30d3\u30b9\u6539\u5584\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u5f93\u6765\u306e\u624b\u6cd5\u3068\u6bd4\u3079\u3066\u4f5c\u696d\u8ca0\u8377\u304c\u8efd\u6e1b\u3055\u308c\u3001\u696d\u52d9\u52b9\u7387\u306e\u5411\u4e0a\u3084\u30b3\u30b9\u30c8\u524a\u6e1b\u3068\u3044\u3063\u305f\u5b9f\u969b\u306e\u4fbf\u76ca\u304c\u5f97\u3089\u308c\u308b\u3068\u671f\u5f85\u3055\u308c\u3066\u3044\u307e\u3059\u3002\"\n}\n",
      "sampling_num": 30,
      "workers": 30,
      "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom pydantic import BaseModel, Field\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\n\n\n@dataclass\nclass ClusterColumns:\n    \"\"\"\u540c\u4e00\u968e\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u95a2\u9023\u306e\u30ab\u30e9\u30e0\u540d\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9\"\"\"\n\n    id: str\n    label: str\n    description: str\n\n    @classmethod\n    def from_id_column(cls, id_column: str) -> \"ClusterColumns\":\n        \"\"\"ID\u5217\u540d\u304b\u3089\u95a2\u9023\u3059\u308b\u30ab\u30e9\u30e0\u540d\u3092\u751f\u6210\"\"\"\n        return cls(\n            id=id_column,\n            label=id_column.replace(\"-id\", \"-label\"),\n            description=id_column.replace(\"-id\", \"-description\"),\n        )\n\n\n@dataclass\nclass ClusterValues:\n    \"\"\"\u5bfe\u8c61\u30af\u30e9\u30b9\u30bf\u306elabel/description\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9\"\"\"\n\n    label: str\n    description: str\n\n    def to_prompt_text(self) -> str:\n        return f\"- {self.label}: {self.description}\"\n\n\ndef hierarchical_merge_labelling(config: dict) -> None:\n    \"\"\"\u968e\u5c64\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306e\u7d50\u679c\u306b\u5bfe\u3057\u3066\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n            - output_dir: \u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\n            - hierarchical_merge_labelling: \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u306e\u8a2d\u5b9a\n                - sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\n                - prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n                - model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n                - workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n            - provider: LLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    merge_path = f\"outputs/{dataset}/hierarchical_merge_labels.csv\"\n    clusters_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_initial_labels.csv\")\n\n    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)\n    # \u30dc\u30c8\u30e0\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u30fb\u8aac\u660e\u3068\u30af\u30e9\u30b9\u30bfid\u4ed8\u304d\u306e\u5404argument\u3092\u5165\u529b\u3057\u3001\u5404\u968e\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\u30fb\u8aac\u660e\u3092\u751f\u6210\u3057\u3001argument\u306b\u4ed8\u3051\u305fdf\u3092\u4f5c\u6210\n    merge_result_df = merge_labelling(\n        clusters_df=clusters_df,\n        cluster_id_columns=sorted(cluster_id_columns, reverse=True),\n        config=config,\n    )\n    # \u4e0a\u8a18\u306edf\u304b\u3089\u5404\u30af\u30e9\u30b9\u30bf\u306elevel, id, label, description, value\u3092\u53d6\u5f97\u3057\u3066df\u3092\u4f5c\u6210\n    melted_df = melt_cluster_data(merge_result_df)\n    # \u4e0a\u8a18\u306edf\u306b\u89aa\u5b50\u95a2\u4fc2\u3092\u8ffd\u52a0\n    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)\n    melted_df = melted_df.merge(parent_child_df, on=[\"level\", \"id\"], how=\"left\")\n    density_df = calculate_cluster_density(melted_df, config)\n    density_df.to_csv(merge_path, index=False)\n\n\ndef _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u9593\u306e\u89aa\u5b50\u95a2\u4fc2\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3059\u308b\n\n    Args:\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        cluster_id_columns: \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n\n    Returns:\n        \u89aa\u5b50\u95a2\u4fc2\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u60c5\u5831\u3092\u542b\u3080DataFrame\n    \"\"\"\n    results = []\n    top_cluster_column = cluster_id_columns[0]\n    top_cluster_values = df[top_cluster_column].unique()\n    for c in top_cluster_values:\n        results.append(\n            {\n                \"level\": 1,\n                \"id\": c,\n                \"parent\": \"0\",  # aggregation\u3067\u8ffd\u52a0\u3059\u308b\u5168\u4f53\u30af\u30e9\u30b9\u30bf\u306eid\n            }\n        )\n\n    for idx in range(len(cluster_id_columns) - 1):\n        current_column = cluster_id_columns[idx]\n        children_column = cluster_id_columns[idx + 1]\n        current_level = current_column.replace(\"-id\", \"\").replace(\"cluster-level-\", \"\")\n        # \u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bfid\n        current_cluster_values = df[current_column].unique()\n        for current_id in current_cluster_values:\n            children_ids = df.loc[df[current_column] == current_id, children_column].unique()\n            for child_id in children_ids:\n                results.append(\n                    {\n                        \"level\": int(current_level) + 1,\n                        \"id\": child_id,\n                        \"parent\": current_id,\n                    }\n                )\n    return pd.DataFrame(results)\n\n\ndef _filter_id_columns(columns: list[str]) -> list[str]:\n    \"\"\"\u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\n\n    Args:\n        columns: \u5168\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n\n    Returns:\n        \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    return [col for col in columns if col.startswith(\"cluster-level-\") and col.endswith(\"-id\")]\n\n\ndef melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u30c7\u30fc\u30bf\u3092\u884c\u5f62\u5f0f\u306b\u5909\u63db\u3059\u308b\n\n    cluster-level-n-(id|label|description) \u3092\u884c\u5f62\u5f0f (level, id, label, description, value) \u306b\u307e\u3068\u3081\u308b\u3002\n    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] \u3092 [level, id, label, description, value(\u4ef6\u6570)] \u306b\u5909\u63db\u3059\u308b\u3002\n\n    Args:\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n\n    Returns:\n        \u884c\u5f62\u5f0f\u306b\u5909\u63db\u3055\u308c\u305fDataFrame\n    \"\"\"\n    id_columns: list[str] = _filter_id_columns(df.columns)\n    levels: set[int] = {int(col.replace(\"cluster-level-\", \"\").replace(\"-id\", \"\")) for col in id_columns}\n    all_rows: list[dict] = []\n\n    # level\u3054\u3068\u306b\u5404\u30af\u30e9\u30b9\u30bf\u306e\u51fa\u73fe\u4ef6\u6570\u3092\u96c6\u8a08\u30fb\u7e26\u6301\u3061\u306b\u3059\u308b\n    for level in levels:\n        cluster_columns = ClusterColumns.from_id_column(f\"cluster-level-{level}-id\")\n        # \u30af\u30e9\u30b9\u30bfid\u3054\u3068\u306e\u4ef6\u6570\u96c6\u8a08\n        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name=\"value\")\n\n        level_unique_val_df = df[\n            [cluster_columns.id, cluster_columns.label, cluster_columns.description]\n        ].drop_duplicates()\n        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how=\"left\")\n        level_unique_vals = [\n            {\n                \"level\": level,\n                \"id\": row[cluster_columns.id],\n                \"label\": row[cluster_columns.label],\n                \"description\": row[cluster_columns.description],\n                \"value\": row[\"value\"],\n            }\n            for _, row in level_unique_val_df.iterrows()\n        ]\n        all_rows.extend(level_unique_vals)\n    return pd.DataFrame(all_rows)\n\n\ndef merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:\n    \"\"\"\u968e\u5c64\u7684\u306a\u30af\u30e9\u30b9\u30bf\u306e\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        clusters_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        cluster_id_columns: \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n\n    Returns:\n        \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080DataFrame\n    \"\"\"\n    for idx in tqdm(range(len(cluster_id_columns) - 1)):\n        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])\n        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])\n\n        process_fn = partial(\n            process_merge_labelling,\n            result_df=clusters_df,\n            current_columns=current_columns,\n            previous_columns=previous_columns,\n            config=config,\n        )\n\n        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())\n        with ThreadPoolExecutor(max_workers=config[\"hierarchical_merge_labelling\"][\"workers\"]) as executor:\n            responses = list(\n                tqdm(\n                    executor.map(process_fn, current_cluster_ids),\n                    total=len(current_cluster_ids),\n                )\n            )\n\n        current_result_df = pd.DataFrame(responses)\n        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])\n    return clusters_df\n\n\nclass LabellingFromat(BaseModel):\n    \"\"\"\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u5b9a\u7fa9\u3059\u308b\"\"\"\n\n    label: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\")\n    description: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\")\n\n\ndef process_merge_labelling(\n    target_cluster_id: str,\n    result_df: pd.DataFrame,\n    current_columns: ClusterColumns,\n    previous_columns: ClusterColumns,\n    config,\n):\n    \"\"\"\u500b\u5225\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        target_cluster_id: \u51e6\u7406\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u30bfID\n        result_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        current_columns: \u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306e\u30ab\u30e9\u30e0\u60c5\u5831\n        previous_columns: \u524d\u306e\u30ec\u30d9\u30eb\u306e\u30ab\u30e9\u30e0\u60c5\u5831\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n\n    Returns:\n        \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080\u8f9e\u66f8\n    \"\"\"\n\n    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:\n        \"\"\"\u524d\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\"\"\"\n        previous_records = df[df[current_columns.id] == target_cluster_id][\n            [previous_columns.label, previous_columns.description]\n        ].drop_duplicates()\n        previous_values = [\n            ClusterValues(\n                label=row[previous_columns.label],\n                description=row[previous_columns.description],\n            )\n            for _, row in previous_records.iterrows()\n        ]\n        return previous_values\n\n    previous_values = filter_previous_values(result_df, previous_columns)\n    if len(previous_values) == 1:\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: previous_values[0].label,\n            current_columns.description: previous_values[0].description,\n        }\n    elif len(previous_values) == 0:\n        raise ValueError(f\"\u30af\u30e9\u30b9\u30bf {target_cluster_id} \u306b\u306f\u524d\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u304c\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\")\n\n    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]\n    sampling_num = min(\n        config[\"hierarchical_merge_labelling\"][\"sampling_num\"],\n        len(current_cluster_data),\n    )\n    sampled_data = current_cluster_data.sample(sampling_num)\n    sampled_argument_text = \"\\n\".join(sampled_data[\"argument\"].values)\n    cluster_text = \"\\n\".join([value.to_prompt_text() for value in previous_values])\n    messages = [\n        {\"role\": \"system\", \"content\": config[\"hierarchical_merge_labelling\"][\"prompt\"]},\n        {\n            \"role\": \"user\",\n            \"content\": \"\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\\n\" + cluster_text + \"\\n\" + \"\u30af\u30e9\u30b9\u30bf\u306e\u610f\u898b\\n\" + sampled_argument_text,\n        },\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=config[\"hierarchical_merge_labelling\"][\"model\"],\n            json_schema=LabellingFromat,\n            provider=config.get(\"provider\", \"openai\"),\n            local_llm_address=config.get(\"local_llm_address\"),\n        )\n        response_json = json.loads(response) if isinstance(response, str) else response\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: response_json.get(\"label\", \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n            current_columns.description: response_json.get(\"description\", \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n        }\n    except Exception as e:\n        print(f\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f: {e}\")\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n            current_columns.description: \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n        }\n\n\ndef calculate_cluster_density(melted_df: pd.DataFrame, config: dict):\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u5185\u306e\u5bc6\u5ea6\u8a08\u7b97\"\"\"\n    hierarchical_cluster_df = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n\n    densities = []\n    for level, c_id in zip(melted_df[\"level\"], melted_df[\"id\"], strict=False):\n        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f\"cluster-level-{level}-id\"] == c_id][\n            [\"x\", \"y\"]\n        ].values\n        density = calculate_density(cluster_embeds)\n        densities.append(density)\n\n    # \u5bc6\u5ea6\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\n    melted_df[\"density\"] = densities\n    melted_df[\"density_rank\"] = melted_df.groupby(\"level\")[\"density\"].rank(ascending=False, method=\"first\")\n    melted_df[\"density_rank_percentile\"] = melted_df.groupby(\"level\")[\"density_rank\"].transform(lambda x: x / len(x))\n    return melted_df\n\n\ndef calculate_density(embeds: np.ndarray):\n    \"\"\"\u5e73\u5747\u8ddd\u96e2\u306b\u57fa\u3065\u3044\u3066\u5bc6\u5ea6\u3092\u8a08\u7b97\"\"\"\n    center = np.mean(embeds, axis=0)\n    distances = np.linalg.norm(embeds - center, axis=1)\n    avg_distance = np.mean(distances)\n    density = 1 / (avg_distance + 1e-10)\n    return density\n",
      "model": "gpt-4o-mini"
    },
    "hierarchical_overview": {
      "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u306e\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n\u30c1\u30fc\u30e0\u306f\u7279\u5b9a\u306e\u30c6\u30fc\u30de\u306b\u95a2\u3057\u3066\u30d1\u30d6\u30ea\u30c3\u30af\u30fb\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u5b9f\u65bd\u3057\u3001\u7570\u306a\u308b\u9078\u629e\u80a2\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u3092\u5206\u6790\u3057\u59cb\u3081\u3066\u3044\u307e\u3059\u3002\n\u3053\u308c\u304b\u3089\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306e\u30ea\u30b9\u30c8\u3068\u305d\u306e\u7c21\u5358\u306a\u5206\u6790\u304c\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u3001\u8abf\u67fb\u7d50\u679c\u306e\u7c21\u6f54\u306a\u8981\u7d04\u3092\u8fd4\u3059\u3053\u3068\u3067\u3059\u3002\u8981\u7d04\u306f\u975e\u5e38\u306b\u7c21\u6f54\u306b\uff08\u6700\u5927\u30671\u6bb5\u843d\u3001\u6700\u59274\u6587\uff09\u307e\u3068\u3081\u3001\u7121\u610f\u5473\u306a\u8a00\u8449\u3092\u907f\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306f\u65e5\u672c\u8a9e\u3067\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n",
      "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport json\nimport re\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\n\nfrom services.llm import request_to_chat_openai\n\n\nclass OverviewResponse(BaseModel):\n    summary: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u5168\u4f53\u7684\u306a\u8981\u7d04\")\n\n\ndef hierarchical_overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_overview.txt\"\n\n    hierarchical_label_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_merge_labels.csv\")\n\n    prompt = config[\"hierarchical_overview\"][\"prompt\"]\n    model = config[\"hierarchical_overview\"][\"model\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    # TODO: level1\u3067\u56fa\u5b9a\u306b\u3057\u3066\u3044\u308b\u304c\u3001\u8a2d\u5b9a\u3067\u5909\u3048\u3089\u308c\u308b\u3088\u3046\u306b\u3059\u308b\n    target_level = 1\n    target_records = hierarchical_label_df[hierarchical_label_df[\"level\"] == target_level]\n    ids = target_records[\"id\"].to_list()\n    labels = target_records[\"label\"].to_list()\n    descriptions = target_records[\"description\"].to_list()\n    target_records.set_index(\"id\", inplace=True)\n\n    input_text = \"\"\n    for i, _ in enumerate(ids):\n        input_text += f\"# Cluster {i}/{len(ids)}: {labels[i]}\\n\\n\"\n        input_text += descriptions[i] + \"\\n\\n\"\n\n    messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": input_text}]\n    response = request_to_chat_openai(\n        messages=messages,\n        model=model,\n        provider=provider,\n        local_llm_address=config.get(\"local_llm_address\"),\n        json_schema=OverviewResponse,\n    )\n\n    try:\n        # structured output\u3068\u3057\u3066\u30d1\u30fc\u30b9\u3067\u304d\u308b\u306a\u3089\u51e6\u7406\u3059\u308b\n        if isinstance(response, dict):\n            parsed_response = response\n        else:\n            parsed_response = json.loads(response)\n\n        with open(path, \"w\") as file:\n            file.write(parsed_response[\"summary\"])\n\n    except Exception:\n        # think\u30bf\u30b0\u304c\u51fa\u529b\u3055\u308c\u308bReasoning\u30e2\u30c7\u30eb\u7528\u306b\u3001think\u30bf\u30b0\u3092\u9664\u53bb\u3059\u308b\n        thinking_removed = re.sub(\n            r\"<think\\b[^>]*>.*?</think>\",\n            \"\",\n            response,\n            flags=re.DOTALL,\n        )\n\n        with open(path, \"w\") as file:\n            file.write(thinking_removed)\n",
      "model": "gpt-4o-mini"
    },
    "hierarchical_aggregation": {
      "sampling_num": 30,
      "hidden_properties": {},
      "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import TypedDict\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\nclass Argument(TypedDict):\n    arg_id: str\n    argument: str\n    comment_id: str\n    x: float\n    y: float\n    p: float\n    cluster_ids: list[str]\n\n\nclass Cluster(TypedDict):\n    level: int\n    id: str\n    label: str\n    takeaway: str\n    value: int\n    parent: str\n    density_rank_percentile: float | None\n\n\ndef hierarchical_aggregation(config):\n    path = f\"outputs/{config['output_dir']}/hierarchical_result.json\"\n    results = {\n        \"arguments\": [],\n        \"clusters\": [],\n        \"comments\": {},\n        \"propertyMap\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n    arg_num = len(arguments)\n    relation_df = pd.read_csv(f\"outputs/{config['output_dir']}/relations.csv\")\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_merge_labels.csv\")\n\n    hidden_properties_map: dict[str, list[str]] = config[\"hierarchical_aggregation\"][\"hidden_properties\"]\n\n    results[\"arguments\"] = _build_arguments(clusters)\n    results[\"clusters\"] = _build_cluster_value(labels, arg_num)\n    # NOTE: \u5c5e\u6027\u306b\u5fdc\u3058\u305f\u30b3\u30e1\u30f3\u30c8\u30d5\u30a3\u30eb\u30bf\u6a5f\u80fd\u304c\u5b9f\u88c5\u3055\u308c\u3066\u304a\u3089\u305a\u3001\u5168\u3066\u306e\u30b3\u30e1\u30f3\u30c8\u304c\u542b\u307e\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\n    # results[\"comments\"] = _build_comments_value(\n    #     comments, arguments, hidden_properties_map\n    # )\n    results[\"comment_num\"] = len(comments)\n    results[\"translations\"] = _build_translations(config)\n    # \u5c5e\u6027\u60c5\u5831\u306e\u30ab\u30e9\u30e0\u306f\u3001\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u6307\u5b9a\u3057\u305f\u30ab\u30e9\u30e0\u3068classification\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u3092\u5408\u308f\u305b\u305f\u3082\u306e\n    results[\"propertyMap\"] = _build_property_map(arguments, hidden_properties_map, config)\n\n    with open(f\"outputs/{config['output_dir']}/hierarchical_overview.txt\") as f:\n        overview = f.read()\n    print(\"overview\")\n    print(overview)\n    results[\"overview\"] = overview\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2, ensure_ascii=False)\n    # TODO: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ed\u30b8\u30c3\u30af\u3092\u5b9f\u88c5\u3057\u305f\u3044\u304c\u3001\u73fe\u72b6\u306f\u5168\u4ef6\u62bd\u51fa\n    create_custom_intro(config)\n    if config[\"is_pubcom\"]:\n        add_original_comments(labels, arguments, relation_df, clusters, config)\n\n\ndef create_custom_intro(config):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/hierarchical_result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n    processed_num = min(input_count, config[\"extraction\"][\"limit\"])\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n\u5206\u6790\u5bfe\u8c61\u3068\u306a\u3063\u305f\u30c7\u30fc\u30bf\u306e\u4ef6\u6570\u306f{processed_num}\u4ef6\u3067\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066OpenAI API\u3092\u7528\u3044\u3066{args_count}\u4ef6\u306e\u610f\u898b\uff08\u8b70\u8ad6\uff09\u3092\u62bd\u51fa\u3057\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u3002\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)\n\n    with open(result_path) as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2, ensure_ascii=False)\n\n\ndef add_original_comments(labels, arguments, relation_df, clusters, config):\n    # \u5927\u30ab\u30c6\u30b4\u30ea\uff08cluster-level-1\uff09\u306b\u8a72\u5f53\u3059\u308b\u30e9\u30d9\u30eb\u3060\u3051\u62bd\u51fa\n    labels_lv1 = labels[labels[\"level\"] == 1][[\"id\", \"label\"]].rename(\n        columns={\"id\": \"cluster-level-1-id\", \"label\": \"category_label\"}\n    )\n\n    # arguments \u3068 clusters \u3092\u30de\u30fc\u30b8\uff08\u30ab\u30c6\u30b4\u30ea\u60c5\u5831\u4ed8\u4e0e\uff09\n    merged = arguments.merge(clusters[[\"arg-id\", \"cluster-level-1-id\"]], on=\"arg-id\").merge(\n        labels_lv1, on=\"cluster-level-1-id\", how=\"left\"\n    )\n\n    # relation_df \u3068\u7d50\u5408\n    merged = merged.merge(relation_df, on=\"arg-id\", how=\"left\")\n\n    # \u5143\u30b3\u30e1\u30f3\u30c8\u53d6\u5f97\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    comments[\"comment-id\"] = comments[\"comment-id\"].astype(str)\n    merged[\"comment-id\"] = merged[\"comment-id\"].astype(str)\n\n    # \u5143\u30b3\u30e1\u30f3\u30c8\u672c\u6587\u306a\u3069\u3068\u30de\u30fc\u30b8\n    final_df = merged.merge(comments, on=\"comment-id\", how=\"left\")\n\n    # \u5fc5\u8981\u30ab\u30e9\u30e0\u306e\u307f\u6574\u5f62\n    final_cols = [\"comment-id\", \"comment-body\", \"arg-id\", \"argument\", \"cluster-level-1-id\", \"category_label\"]\n    for col in [\"source\", \"url\"]:\n        if col in comments.columns:\n            final_cols.append(col)\n\n    final_df = final_df[final_cols]\n    final_df = final_df.rename(\n        columns={\n            \"cluster-level-1-id\": \"category_id\",\n            \"category_label\": \"category\",\n            \"arg-id\": \"arg_id\",\n            \"argument\": \"argument\",\n            \"comment-body\": \"original-comment\",\n        }\n    )\n\n    # \u4fdd\u5b58\n    final_df.to_csv(f\"outputs/{config['output_dir']}/final_result_with_comments.csv\", index=False)\n\n\ndef _build_arguments(clusters: pd.DataFrame) -> list[Argument]:\n    cluster_columns = [col for col in clusters.columns if col.startswith(\"cluster-level-\") and \"id\" in col]\n\n    arguments: list[Argument] = []\n    for _, row in clusters.iterrows():\n        cluster_ids = [\"0\"]\n        for cluster_column in cluster_columns:\n            cluster_ids.append(row[cluster_column])\n        argument: Argument = {\n            \"arg_id\": row[\"arg-id\"],\n            \"argument\": row[\"argument\"],\n            \"x\": row[\"x\"],\n            \"y\": row[\"y\"],\n            \"p\": 0,  # NOTE: \u4e00\u65e6\u5168\u90e80\u3067\u3044\u308c\u308b\n            \"cluster_ids\": cluster_ids,\n        }\n        arguments.append(argument)\n    return arguments\n\n\ndef _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:\n    results: list[Cluster] = [\n        Cluster(\n            level=0,\n            id=\"0\",\n            label=\"\u5168\u4f53\",\n            takeaway=\"\",\n            value=total_num,\n            parent=\"\",\n            density_rank_percentile=0,\n        )\n    ]\n\n    for _, melted_label in melted_labels.iterrows():\n        cluster_value = Cluster(\n            level=melted_label[\"level\"],\n            id=melted_label[\"id\"],\n            label=melted_label[\"label\"],\n            takeaway=melted_label[\"description\"],\n            value=melted_label[\"value\"],\n            parent=melted_label.get(\"parent\", \"\u5168\u4f53\"),\n            density_rank_percentile=melted_label.get(\"density_rank_percentile\"),\n        )\n        results.append(cluster_value)\n    return results\n\n\ndef _build_comments_value(\n    comments: pd.DataFrame,\n    arguments: pd.DataFrame,\n    hidden_properties_map: dict[str, list[str]],\n):\n    comment_dict: dict[str, dict[str, str]] = {}\n    useful_comment_ids = set(arguments[\"comment-id\"].values)\n    for _, row in comments.iterrows():\n        id = row[\"comment-id\"]\n        if id in useful_comment_ids:\n            res = {\"comment\": row[\"comment-body\"]}\n            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())\n            if should_skip:\n                continue\n            comment_dict[str(id)] = res\n\n    return comment_dict\n\n\ndef _build_translations(config):\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        return json.loads(translations)\n    return {}\n\n\ndef _build_property_map(\n    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict\n) -> dict[str, dict[str, str]]:\n    property_columns = list(hidden_properties_map.keys()) + list(config[\"extraction\"][\"categories\"].keys())\n    property_map = defaultdict(dict)\n\n    # \u6307\u5b9a\u3055\u308c\u305f property_columns \u304c arguments \u306b\u5b58\u5728\u3059\u308b\u304b\u30c1\u30a7\u30c3\u30af\n    missing_cols = [col for col in property_columns if col not in arguments.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"\u6307\u5b9a\u3055\u308c\u305f\u30ab\u30e9\u30e0 {missing_cols} \u304c args.csv \u306b\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\"\n            \"\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30ebaggregation / hidden_properties\u304b\u3089\u8a72\u5f53\u30ab\u30e9\u30e0\u3092\u53d6\u308a\u9664\u3044\u3066\u304f\u3060\u3055\u3044\u3002\"\n        )\n\n    for prop in property_columns:\n        for arg_id, row in arguments.iterrows():\n            # LLM\u306b\u3088\u308bcategory classification\u304c\u3046\u307e\u304f\u884c\u304b\u305a\u3001NaN\u306e\u5834\u5408\u306fNone\u306b\u3059\u308b\n            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None\n    return property_map\n"
    },
    "output_dir": "test-report",
    "skip-interaction": true,
    "without-html": true,
    "embedding": {
      "model": "text-embedding-3-small",
      "source_code": "import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n    is_embedded_at_local = config[\"is_embedded_at_local\"]\n    # print(\"start embedding\")\n    # print(f\"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}\")\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model, is_embedded_at_local)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"
    },
    "hierarchical_visualization": {
      "replacements": [],
      "source_code": "import subprocess\n\n\ndef hierarchical_visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../report\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"
    },
    "plan": [
      {
        "step": "extraction",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "embedding",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_clustering",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_initial_labelling",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_merge_labelling",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_overview",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_aggregation",
        "run": true,
        "reason": "not trace of previous run"
      },
      {
        "step": "hierarchical_visualization",
        "run": false,
        "reason": "skipping html output"
      }
    ],
    "status": "completed",
    "start_time": "2025-05-13T07:57:00.334787",
    "completed_jobs": [
      {
        "step": "extraction",
        "completed": "2025-05-13T07:57:37.057646",
        "duration": 36.720279,
        "params": {
          "prompt": "\u3042\u306a\u305f\u306f\u5c02\u9580\u7684\u306a\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u4e0e\u3048\u3089\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u3001\u610f\u898b\u3092\u62bd\u51fa\u3057\u3066\u6574\u7406\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n# \u6307\u793a\n* \u5165\u51fa\u529b\u306e\u4f8b\u306b\u8a18\u8f09\u3057\u305f\u3088\u3046\u306a\u5f62\u5f0f\u3067\u6587\u5b57\u5217\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\n  * \u5fc5\u8981\u306a\u5834\u5408\u306f2\u3064\u306e\u5225\u500b\u306e\u610f\u898b\u306b\u5206\u5272\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u591a\u304f\u306e\u5834\u5408\u306f1\u3064\u306e\u8b70\u8ad6\u306b\u307e\u3068\u3081\u308b\u65b9\u304c\u671b\u307e\u3057\u3044\u3067\u3059\u3002\n* \u6574\u7406\u3057\u305f\u610f\u898b\u306f\u65e5\u672c\u8a9e\u3067\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n## \u5165\u51fa\u529b\u306e\u4f8b\n/human\n\nAI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u5168\u4f53\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u30c6\u30af\u30ce\u30ed\u30b8\u30fc\u306f\u3001\u305d\u306e\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u5168\u4f53\u306b\u304a\u3051\u308b\u74b0\u5883\u8ca0\u8377\u3092\u524a\u6e1b\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u5f53\u3066\u3066\u958b\u767a\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\"\n  ]\n}\n\n/human\n\nAI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u3001\u5e02\u6c11\u3092\u6559\u80b2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u307e\u305f\u3001\u6559\u80b2\u3067\u304d\u308b\u4eba\u6750\u3092\u990a\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u306e\u80fd\u529b\u3001\u9650\u754c\u3001\u502b\u7406\u7684\u8003\u616e\u4e8b\u9805\u306b\u3064\u3044\u3066\u3001\u5e02\u6c11\u3092\u6559\u80b2\u3059\u3079\u304d\",\n    \"AI\u306b\u95a2\u3059\u308b\u6559\u80b2\u3092\u3067\u304d\u308b\u4eba\u6750\u3092\u990a\u6210\u3059\u3079\u304d\"\n  ]\n}\n\n/human\n\nAI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3001\u7121\u99c4\u3084\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u307e\u3059\u3002\n\n/ai\n\n{\n  \"extractedOpinionList\": [\n    \"AI\u306f\u30a8\u30cd\u30eb\u30ae\u30fc\u30b0\u30ea\u30c3\u30c9\u3092\u6700\u9069\u5316\u3057\u3066\u70ad\u7d20\u6392\u51fa\u3092\u524a\u6e1b\u3067\u304d\u308b\"\n  ]\n}\n",
          "workers": 30,
          "limit": 254,
          "properties": [],
          "categories": {},
          "category_batch_size": 5,
          "source_code": "import concurrent.futures\nimport json\nimport logging\nimport re\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\nfrom tqdm import tqdm\n\nfrom services.category_classification import classify_args\nfrom services.llm import request_to_chat_openai\nfrom services.parse_json_list import parse_extraction_response\nfrom utils import update_progress\n\nCOMMA_AND_SPACE_AND_RIGHT_BRACKET = re.compile(r\",\\s*(\\])\")\n\n\nclass ExtractionResponse(BaseModel):\n    extractedOpinionList: list[str] = Field(..., description=\"\u62bd\u51fa\u3057\u305f\u610f\u898b\u306e\u30ea\u30b9\u30c8\")\n\n\ndef _validate_property_columns(property_columns: list[str], comments: pd.DataFrame) -> None:\n    if not all(property in comments.columns for property in property_columns):\n        raise ValueError(f\"Properties {property_columns} not found in comments. Columns are {comments.columns}\")\n\n\ndef extraction(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/args.csv\"\n    model = config[\"extraction\"][\"model\"]\n    prompt = config[\"extraction\"][\"prompt\"]\n    workers = config[\"extraction\"][\"workers\"]\n    limit = config[\"extraction\"][\"limit\"]\n    property_columns = config[\"extraction\"][\"properties\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    # \u30ab\u30e9\u30e0\u540d\u3060\u3051\u3092\u8aad\u307f\u8fbc\u307f\u3001\u5fc5\u8981\u306a\u30ab\u30e9\u30e0\u304c\u542b\u307e\u308c\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\", nrows=0)\n    _validate_property_columns(property_columns, comments)\n    # \u30a8\u30e9\u30fc\u304c\u51fa\u306a\u304b\u3063\u305f\u5834\u5408\u3001\u3059\u3079\u3066\u306e\u884c\u3092\u8aad\u307f\u8fbc\u3080\n    comments = pd.read_csv(\n        f\"inputs/{config['input']}.csv\", usecols=[\"comment-id\", \"comment-body\"] + config[\"extraction\"][\"properties\"]\n    )\n    comment_ids = (comments[\"comment-id\"].values)[:limit]\n    comments.set_index(\"comment-id\", inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n\n    argument_map = {}\n    relation_rows = []\n\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i : i + workers]\n        batch_inputs = [comments.loc[id][\"comment-body\"] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers, provider, config.get(\"local_llm_address\"))\n\n        for comment_id, extracted_args in zip(batch, batch_results, strict=False):\n            for j, arg in enumerate(extracted_args):\n                if arg not in argument_map:\n                    # argument\u30c6\u30fc\u30d6\u30eb\u306b\u8ffd\u52a0\n                    arg_id = f\"A{comment_id}_{j}\"\n                    argument_map[arg] = {\n                        \"arg-id\": arg_id,\n                        \"argument\": arg,\n                    }\n                else:\n                    arg_id = argument_map[arg][\"arg-id\"]\n\n                # relation\u30c6\u30fc\u30d6\u30eb\u306bcomment\u3068arg\u306e\u95a2\u4fc2\u3092\u8ffd\u52a0\n                relation_row = {\n                    \"arg-id\": arg_id,\n                    \"comment-id\": comment_id,\n                }\n                relation_rows.append(relation_row)\n\n        update_progress(config, incr=len(batch))\n\n    # DataFrame\u5316\n    results = pd.DataFrame(argument_map.values())\n    relation_df = pd.DataFrame(relation_rows)\n\n    if results.empty:\n        raise RuntimeError(\"result is empty, maybe bad prompt\")\n\n    classification_categories = config[\"extraction\"][\"categories\"]\n    if classification_categories:\n        results = classify_args(results, config, workers)\n\n    results.to_csv(path, index=False)\n    # comment-id\u3068arg-id\u306e\u95a2\u4fc2\u3092\u4fdd\u5b58\n    relation_df.to_csv(f\"outputs/{dataset}/relations.csv\", index=False)\n\n\nlogging.basicConfig(level=logging.ERROR)\n\n\ndef extract_batch(batch, prompt, model, workers, provider=\"openai\", local_llm_address=None):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures_with_index = [\n            (i, executor.submit(extract_arguments, input, prompt, model, provider, local_llm_address))\n            for i, input in enumerate(batch)\n        ]\n\n        done, not_done = concurrent.futures.wait([f for _, f in futures_with_index], timeout=30)\n        results = [[] for _ in range(len(batch))]\n\n        for _, future in futures_with_index:\n            if future in not_done and not future.cancelled():\n                future.cancel()\n\n        for i, future in futures_with_index:\n            if future in done:\n                try:\n                    result = future.result()\n                    results[i] = result\n                except Exception as e:\n                    logging.error(f\"Task {future} failed with error: {e}\")\n                    results[i] = []\n        return results\n\n\ndef extract_arguments(input, prompt, model, provider=\"openai\", local_llm_address=None):\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=model,\n            is_json=False,\n            json_schema=ExtractionResponse,\n            provider=provider,\n            local_llm_address=local_llm_address,\n        )\n        items = parse_extraction_response(response)\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        print(\"Silently giving up on trying to generate valid list.\")\n        return []\n",
          "model": "gpt-4o-mini"
        }
      },
      {
        "step": "embedding",
        "completed": "2025-05-13T07:57:40.265961",
        "duration": 3.2061,
        "params": {
          "model": "text-embedding-3-small",
          "source_code": "import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n    is_embedded_at_local = config[\"is_embedded_at_local\"]\n    # print(\"start embedding\")\n    # print(f\"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}\")\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model, is_embedded_at_local)\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"
        }
      },
      {
        "step": "hierarchical_clustering",
        "completed": "2025-05-13T07:57:48.706276",
        "duration": 8.438271,
        "params": {
          "cluster_nums": [
            6,
            36
          ],
          "source_code": "\"\"\"Cluster the arguments using UMAP + HDBSCAN and GPT-4.\"\"\"\n\nfrom importlib import import_module\n\nimport numpy as np\nimport pandas as pd\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import KMeans\n\n\ndef hierarchical_clustering(config):\n    UMAP = import_module(\"umap\").UMAP\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    cluster_nums = config[\"hierarchical_clustering\"][\"cluster_nums\"]\n\n    n_samples = embeddings_array.shape[0]\n    # \u30c7\u30d5\u30a9\u30eb\u30c8\u8a2d\u5b9a\u306f15\n    default_n_neighbors = 15\n\n    # \u30c6\u30b9\u30c8\u7b49\u30b5\u30f3\u30d7\u30eb\u304c\u5c11\u306a\u3059\u304e\u308b\u5834\u5408\u3001n_neighbors\u306e\u8a2d\u5b9a\u5024\u3092\u4e0b\u3052\u308b\n    if n_samples <= default_n_neighbors:\n        n_neighbors = max(2, n_samples - 1)  # \u6700\u4f4e2\u4ee5\u4e0a\n    else:\n        n_neighbors = default_n_neighbors\n\n    umap_model = UMAP(random_state=42, n_components=2, n_neighbors=n_neighbors)\n    # TODO \u8a73\u7d30\u30a8\u30e9\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u52a0\u3048\u308b\n    # \u4ee5\u4e0b\u306e\u30a8\u30e9\u30fc\u306e\u5834\u5408\u3001\u304a\u305d\u3089\u304f\u5143\u306e\u610f\u898b\u4ef6\u6570\u304c\u5c11\u306a\u3059\u304e\u308b\u3053\u3068\u304c\u539f\u56e0\n    # TypeError: Cannot use scipy.linalg.eigh for sparse A with k >= N. Use scipy.linalg.eigh(A.toarray()) or reduce k.\n    umap_embeds = umap_model.fit_transform(embeddings_array)\n\n    cluster_results = hierarchical_clustering_embeddings(\n        umap_embeds=umap_embeds,\n        cluster_nums=cluster_nums,\n    )\n    result_df = pd.DataFrame(\n        {\n            \"arg-id\": arguments_df[\"arg-id\"],\n            \"argument\": arguments_df[\"argument\"],\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        }\n    )\n\n    for cluster_level, final_labels in enumerate(cluster_results.values(), start=1):\n        result_df[f\"cluster-level-{cluster_level}-id\"] = [f\"{cluster_level}_{label}\" for label in final_labels]\n\n    result_df.to_csv(path, index=False)\n\n\ndef generate_cluster_count_list(min_clusters: int, max_clusters: int):\n    cluster_counts = []\n    current = min_clusters\n    cluster_counts.append(current)\n\n    if min_clusters == max_clusters:\n        return cluster_counts\n\n    while True:\n        next_double = current * 2\n        next_triple = current * 3\n\n        if next_double >= max_clusters:\n            if cluster_counts[-1] != max_clusters:\n                cluster_counts.append(max_clusters)\n            break\n\n        # \u6b21\u306e\u500d\u306f\u307e\u3060 max_clusters \u306b\u53ce\u307e\u308b\u304c\u30013\u500d\u3060\u3068\u8d85\u3048\u308b\n        # -> (\u6b21\u306e\u500d\u306f\u7d30\u304b\u3059\u304e\u308b\u306e\u3067)\u30b9\u30ad\u30c3\u30d7\u3057\u3066 max_clusters \u306b\u98db\u3076\n        if next_triple > max_clusters:\n            cluster_counts.append(max_clusters)\n            break\n\n        cluster_counts.append(next_double)\n        current = next_double\n\n    return cluster_counts\n\n\ndef merge_clusters_with_hierarchy(\n    cluster_centers: np.ndarray,\n    kmeans_labels: np.ndarray,\n    umap_array: np.ndarray,\n    n_cluster_cut: int,\n):\n    Z = sch.linkage(cluster_centers, method=\"ward\")\n    cluster_labels_merged = sch.fcluster(Z, t=n_cluster_cut, criterion=\"maxclust\")\n\n    n_samples = umap_array.shape[0]\n    final_labels = np.zeros(n_samples, dtype=int)\n\n    for i in range(n_samples):\n        original_label = kmeans_labels[i]\n        final_labels[i] = cluster_labels_merged[original_label]\n\n    return final_labels\n\n\ndef hierarchical_clustering_embeddings(\n    umap_embeds,\n    cluster_nums,\n):\n    # \u6700\u5927\u5206\u5272\u6570\u3067\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u5b9f\u65bd\n    print(\"start initial clustering\")\n    initial_cluster_num = cluster_nums[-1]\n    kmeans_model = KMeans(n_clusters=initial_cluster_num, random_state=42)\n    kmeans_model.fit(umap_embeds)\n    print(\"end initial clustering\")\n\n    results = {}\n    print(\"start hierarchical clustering\")\n    cluster_nums.sort()\n    print(cluster_nums)\n    for n_cluster_cut in cluster_nums[:-1]:\n        print(\"n_cluster_cut: \", n_cluster_cut)\n        final_labels = merge_clusters_with_hierarchy(\n            cluster_centers=kmeans_model.cluster_centers_,\n            kmeans_labels=kmeans_model.labels_,\n            umap_array=umap_embeds,\n            n_cluster_cut=n_cluster_cut,\n        )\n        results[n_cluster_cut] = final_labels\n\n    results[initial_cluster_num] = kmeans_model.labels_\n    print(\"end hierarchical clustering\")\n\n    return results\n"
        }
      },
      {
        "step": "hierarchical_initial_labelling",
        "completed": "2025-05-13T07:57:54.285862",
        "duration": 5.577924,
        "params": {
          "prompt": "\u3042\u306a\u305f\u306fKJ\u6cd5\u304c\u5f97\u610f\u306a\u30c7\u30fc\u30bf\u5206\u6790\u8005\u3067\u3059\u3002user\u306einput\u306f\u30b0\u30eb\u30fc\u30d7\u306b\u96c6\u307e\u3063\u305f\u30e9\u30d9\u30eb\u3067\u3059\u3002\u306a\u305c\u305d\u306e\u30e9\u30d9\u30eb\u304c\u4e00\u3064\u306e\u30b0\u30eb\u30fc\u30d7\u3067\u3042\u308b\u304b\u89e3\u8aac\u3057\u3001\u8868\u672d\uff08label\uff09\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u8868\u672d\u306b\u3064\u3044\u3066\u306f\u3001\u30b0\u30eb\u30fc\u30d7\u5185\u306e\u5177\u4f53\u7684\u306a\u8ad6\u70b9\u3084\u7279\u5fb4\u3092\u53cd\u6620\u3057\u305f\u3001\u5177\u4f53\u6027\u306e\u9ad8\u3044\u540d\u79f0\u3092\u8003\u6848\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306fJSON\u3068\u3057\u3001\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u53c2\u8003\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u5165\u51fa\u529b\n## \u5165\u529b\u4f8b\n- \u624b\u4f5c\u696d\u3067\u306e\u610f\u898b\u5206\u6790\u306f\u6642\u9593\u304c\u304b\u304b\u308a\u3059\u304e\u308b\u3002AI\u3067\u52b9\u7387\u5316\u3067\u304d\u308b\u3068\u5b09\u3057\u3044\n- \u4eca\u306e\u3084\u308a\u65b9\u3060\u3068\u5206\u6790\u306b\u5de5\u6570\u304c\u304b\u304b\u308a\u3059\u304e\u308b\u3051\u3069\u3001AI\u306a\u3089\u30b3\u30b9\u30c8\u3092\u304b\u3051\u305a\u306b\u5206\u6790\u3067\u304d\u305d\u3046\n- AI\u304c\u81ea\u52d5\u3067\u610f\u898b\u3092\u6574\u7406\u3057\u3066\u304f\u308c\u308b\u3068\u697d\u306b\u306a\u3063\u3066\u5b09\u3057\u3044\n\n\n## \u51fa\u529b\u4f8b\n{\n    \"label\": \"AI\u306b\u3088\u308b\u696d\u52d9\u52b9\u7387\u306e\u5927\u5e45\u5411\u4e0a\u3068\u30b3\u30b9\u30c8\u52b9\u7387\u5316\",\n    \"description\": \"\u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u5f93\u6765\u306e\u624b\u4f5c\u696d\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u3068\u6bd4\u8f03\u3057\u3066\u3001AI\u306b\u3088\u308b\u81ea\u52d5\u5316\u3067\u5206\u6790\u30d7\u30ed\u30bb\u30b9\u304c\u52b9\u7387\u5316\u3055\u308c\u3001\u4f5c\u696d\u6642\u9593\u306e\u77ed\u7e2e\u3084\u904b\u7528\u30b3\u30b9\u30c8\u306e\u52b9\u7387\u5316\u304c\u5b9f\u73fe\u3055\u308c\u308b\u70b9\u306b\u5bfe\u3059\u308b\u524d\u5411\u304d\u306a\u8a55\u4fa1\u304c\u4e2d\u5fc3\u3067\u3059\u3002\"\n}\n",
          "sampling_num": 30,
          "workers": 30,
          "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom functools import partial\nfrom typing import TypedDict\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\n\nfrom services.llm import request_to_chat_openai\n\n\nclass LabellingResult(TypedDict):\n    \"\"\"\u5404\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u8868\u3059\u578b\"\"\"\n\n    cluster_id: str  # \u30af\u30e9\u30b9\u30bf\u306eID\n    label: str  # \u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\n    description: str  # \u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\n\n\ndef hierarchical_initial_labelling(config: dict) -> None:\n    \"\"\"\u968e\u5c64\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306e\u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n            - output_dir: \u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\n            - hierarchical_initial_labelling: \u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u306e\u8a2d\u5b9a\n                - sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\n                - prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n                - model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n                - workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n            - provider: LLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_initial_labels.csv\"\n    clusters_argument_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_clusters.csv\")\n\n    cluster_id_columns = [col for col in clusters_argument_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_id_column = cluster_id_columns[-1]\n    sampling_num = config[\"hierarchical_initial_labelling\"][\"sampling_num\"]\n    initial_labelling_prompt = config[\"hierarchical_initial_labelling\"][\"prompt\"]\n    model = config[\"hierarchical_initial_labelling\"][\"model\"]\n    workers = config[\"hierarchical_initial_labelling\"][\"workers\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    initial_label_df = initial_labelling(\n        initial_labelling_prompt,\n        clusters_argument_df,\n        sampling_num,\n        model,\n        workers,\n        provider,\n        config.get(\"local_llm_address\"),\n    )\n    print(\"start initial labelling\")\n    initial_clusters_argument_df = clusters_argument_df.merge(\n        initial_label_df,\n        left_on=initial_cluster_id_column,\n        right_on=\"cluster_id\",\n        how=\"left\",\n    ).rename(\n        columns={\n            \"label\": f\"{initial_cluster_id_column.replace('-id', '')}-label\",\n            \"description\": f\"{initial_cluster_id_column.replace('-id', '')}-description\",\n        }\n    )\n    print(\"end initial labelling\")\n    initial_clusters_argument_df.to_csv(path, index=False)\n\n\ndef initial_labelling(\n    prompt: str,\n    clusters_df: pd.DataFrame,\n    sampling_num: int,\n    model: str,\n    workers: int,\n    provider: str = \"openai\",\n    local_llm_address: str = None,\n) -> pd.DataFrame:\n    \"\"\"\u5404\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u521d\u671f\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        clusters_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        sampling_num: \u5404\u30af\u30e9\u30b9\u30bf\u304b\u3089\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u610f\u898b\u306e\u6570\n        model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n        workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n\n    Returns:\n        \u5404\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080DataFrame\n    \"\"\"\n    cluster_columns = [col for col in clusters_df.columns if col.startswith(\"cluster-level-\")]\n    initial_cluster_column = cluster_columns[-1]\n    cluster_ids = clusters_df[initial_cluster_column].unique()\n    process_func = partial(\n        process_initial_labelling,\n        df=clusters_df,\n        prompt=prompt,\n        sampling_num=sampling_num,\n        target_column=initial_cluster_column,\n        model=model,\n        provider=provider,\n        local_llm_address=local_llm_address,\n    )\n    with ThreadPoolExecutor(max_workers=workers) as executor:\n        results = list(executor.map(process_func, cluster_ids))\n    return pd.DataFrame(results)\n\n\nclass LabellingFromat(BaseModel):\n    \"\"\"\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u5b9a\u7fa9\u3059\u308b\"\"\"\n\n    label: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\")\n    description: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\")\n\n\ndef process_initial_labelling(\n    cluster_id: str,\n    df: pd.DataFrame,\n    prompt: str,\n    sampling_num: int,\n    target_column: str,\n    model: str,\n    provider: str = \"openai\",\n    local_llm_address: str = None,\n) -> LabellingResult:\n    \"\"\"\u500b\u5225\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        cluster_id: \u51e6\u7406\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u30bfID\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u610f\u898b\u306e\u6570\n        target_column: \u30af\u30e9\u30b9\u30bfID\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b\u5217\u540d\n        model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n\n    Returns:\n        \u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\n    \"\"\"\n    cluster_data = df[df[target_column] == cluster_id]\n    sampling_num = min(sampling_num, len(cluster_data))\n    cluster = cluster_data.sample(sampling_num)\n    input = \"\\n\".join(cluster[\"argument\"].values)\n    messages = [\n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": input},\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=model,\n            provider=provider,\n            json_schema=LabellingFromat,\n            local_llm_address=local_llm_address,\n        )\n        response_json = json.loads(response) if isinstance(response, str) else response\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=response_json.get(\"label\", \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n            description=response_json.get(\"description\", \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n        )\n    except Exception as e:\n        print(e)\n        return LabellingResult(\n            cluster_id=cluster_id,\n            label=\"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n            description=\"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n        )\n",
          "model": "gpt-4o-mini"
        }
      },
      {
        "step": "hierarchical_merge_labelling",
        "completed": "2025-05-13T07:57:59.113988",
        "duration": 4.825496,
        "params": {
          "prompt": "\u3042\u306a\u305f\u306f\u30c7\u30fc\u30bf\u5206\u6790\u306e\u30a8\u30ad\u30b9\u30d1\u30fc\u30c8\u3067\u3059\u3002\n\u73fe\u5728\u3001\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u968e\u5c64\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u4e0b\u5c64\u306e\u30af\u30e9\u30b9\u30bf\uff08\u610f\u898b\u30b0\u30eb\u30fc\u30d7\uff09\u306e\u30bf\u30a4\u30c8\u30eb\u3068\u8aac\u660e\u3001\u304a\u3088\u3073\u305d\u308c\u3089\u306e\u30af\u30e9\u30b9\u30bf\u304c\u6240\u5c5e\u3059\u308b\u4e0a\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u4e0e\u3048\u308b\u306e\u3067\u3001\u4e0a\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u306e\u30bf\u30a4\u30c8\u30eb\u3068\u8aac\u660e\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n# \u6307\u793a\n- \u7d71\u5408\u5f8c\u306e\u30af\u30e9\u30b9\u30bf\u540d\u306f\u3001\u7d71\u5408\u524d\u306e\u30af\u30e9\u30b9\u30bf\u540d\u79f0\u3092\u305d\u306e\u307e\u307e\u5f15\u7528\u305b\u305a\u3001\u5185\u5bb9\u306b\u57fa\u3065\u3044\u305f\u65b0\u305f\u306a\u540d\u79f0\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n- \u30bf\u30a4\u30c8\u30eb\u306b\u306f\u3001\u5177\u4f53\u7684\u306a\u4e8b\u8c61\u30fb\u884c\u52d5\uff08\u4f8b\uff1a\u5730\u57df\u3054\u3068\u306e\u8fc5\u901f\u5bfe\u5fdc\u3001\u5fa9\u8208\u8a08\u753b\u306e\u7740\u5b9f\u306a\u9032\u5c55\u3001\u52b9\u679c\u7684\u306a\u60c5\u5831\u5171\u6709\u30fb\u5730\u57df\u5354\u529b\u306a\u3069\uff09\u3092\u542b\u3081\u3066\u304f\u3060\u3055\u3044\n  - \u53ef\u80fd\u306a\u9650\u308a\u5177\u4f53\u7684\u306a\u8868\u73fe\u3092\u7528\u3044\u308b\u3088\u3046\u306b\u3057\u3001\u62bd\u8c61\u7684\u306a\u8868\u73fe\u306f\u907f\u3051\u3066\u304f\u3060\u3055\u3044\n    - \u300c\u591a\u69d8\u306a\u610f\u898b\u300d\u306a\u3069\u306e\u62bd\u8c61\u7684\u306a\u8868\u73fe\u306f\u907f\u3051\u3066\u304f\u3060\u3055\u3044\n- \u51fa\u529b\u4f8b\u306b\u793a\u3057\u305fJSON\u5f62\u5f0f\u3067\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\n\n\n# \u30b5\u30f3\u30d7\u30eb\u306e\u5165\u51fa\u529b\n## \u5165\u529b\u4f8b\n- \u300c\u9867\u5ba2\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u306e\u81ea\u52d5\u96c6\u7d04\u300d: \u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001SNS\u3084\u30aa\u30f3\u30e9\u30a4\u30f3\u30ec\u30d3\u30e5\u30fc\u306a\u3069\u304b\u3089\u96c6\u3081\u305f\u5927\u91cf\u306e\u610f\u898b\u3092AI\u304c\u77ac\u6642\u306b\u89e3\u6790\u3057\u3001\u4f01\u696d\u304c\u5e02\u5834\u306e\u30c8\u30ec\u30f3\u30c9\u3084\u9867\u5ba2\u306e\u8981\u671b\u3092\u5373\u6642\u306b\u628a\u63e1\u3067\u304d\u308b\u70b9\u306b\u3064\u3044\u3066\u306e\u671f\u5f85\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\n- \u300cAI\u306b\u3088\u308b\u696d\u52d9\u52b9\u7387\u306e\u5927\u5e45\u5411\u4e0a\u3068\u30b3\u30b9\u30c8\u52b9\u7387\u5316\u300d: \u3053\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306f\u3001\u5f93\u6765\u306e\u624b\u4f5c\u696d\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u3068\u6bd4\u8f03\u3057\u3066\u3001AI\u306b\u3088\u308b\u81ea\u52d5\u5316\u3067\u5206\u6790\u30d7\u30ed\u30bb\u30b9\u304c\u52b9\u7387\u5316\u3055\u308c\u3001\u4f5c\u696d\u6642\u9593\u306e\u77ed\u7e2e\u3084\u904b\u7528\u30b3\u30b9\u30c8\u306e\u52b9\u7387\u5316\u304c\u5b9f\u73fe\u3055\u308c\u308b\u70b9\u306b\u5bfe\u3059\u308b\u524d\u5411\u304d\u306a\u8a55\u4fa1\u304c\u4e2d\u5fc3\u3067\u3059\u3002\n\n## \u51fa\u529b\u4f8b\n{\n    \"label\": \"AI\u6280\u8853\u306e\u5c0e\u5165\u306b\u3088\u308b\u610f\u898b\u5206\u6790\u306e\u52b9\u7387\u5316\u3078\u306e\u671f\u5f85\",\n    \"description\": \"\u5927\u91cf\u306e\u610f\u898b\u3084\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304b\u3089\u8fc5\u901f\u306b\u6d1e\u5bdf\u3092\u62bd\u51fa\u3067\u304d\u308b\u305f\u3081\u3001\u4f01\u696d\u3084\u81ea\u6cbb\u4f53\u304c\u6d88\u8cbb\u8005\u3084\u5e02\u6c11\u306e\u58f0\u3092\u7684\u78ba\u306b\u628a\u63e1\u3057\u3001\u6226\u7565\u7684\u306a\u610f\u601d\u6c7a\u5b9a\u3084\u30b5\u30fc\u30d3\u30b9\u6539\u5584\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u5f93\u6765\u306e\u624b\u6cd5\u3068\u6bd4\u3079\u3066\u4f5c\u696d\u8ca0\u8377\u304c\u8efd\u6e1b\u3055\u308c\u3001\u696d\u52d9\u52b9\u7387\u306e\u5411\u4e0a\u3084\u30b3\u30b9\u30c8\u524a\u6e1b\u3068\u3044\u3063\u305f\u5b9f\u969b\u306e\u4fbf\u76ca\u304c\u5f97\u3089\u308c\u308b\u3068\u671f\u5f85\u3055\u308c\u3066\u3044\u307e\u3059\u3002\"\n}\n",
          "sampling_num": 30,
          "workers": 30,
          "source_code": "import json\nfrom concurrent.futures import ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom pydantic import BaseModel, Field\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_chat_openai\n\n\n@dataclass\nclass ClusterColumns:\n    \"\"\"\u540c\u4e00\u968e\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u30fc\u95a2\u9023\u306e\u30ab\u30e9\u30e0\u540d\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9\"\"\"\n\n    id: str\n    label: str\n    description: str\n\n    @classmethod\n    def from_id_column(cls, id_column: str) -> \"ClusterColumns\":\n        \"\"\"ID\u5217\u540d\u304b\u3089\u95a2\u9023\u3059\u308b\u30ab\u30e9\u30e0\u540d\u3092\u751f\u6210\"\"\"\n        return cls(\n            id=id_column,\n            label=id_column.replace(\"-id\", \"-label\"),\n            description=id_column.replace(\"-id\", \"-description\"),\n        )\n\n\n@dataclass\nclass ClusterValues:\n    \"\"\"\u5bfe\u8c61\u30af\u30e9\u30b9\u30bf\u306elabel/description\u3092\u7ba1\u7406\u3059\u308b\u30af\u30e9\u30b9\"\"\"\n\n    label: str\n    description: str\n\n    def to_prompt_text(self) -> str:\n        return f\"- {self.label}: {self.description}\"\n\n\ndef hierarchical_merge_labelling(config: dict) -> None:\n    \"\"\"\u968e\u5c64\u7684\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306e\u7d50\u679c\u306b\u5bfe\u3057\u3066\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n            - output_dir: \u51fa\u529b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d\n            - hierarchical_merge_labelling: \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u306e\u8a2d\u5b9a\n                - sampling_num: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6570\n                - prompt: LLM\u3078\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n                - model: \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\n                - workers: \u4e26\u5217\u51e6\u7406\u306e\u30ef\u30fc\u30ab\u30fc\u6570\n            - provider: LLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\n    \"\"\"\n    dataset = config[\"output_dir\"]\n    merge_path = f\"outputs/{dataset}/hierarchical_merge_labels.csv\"\n    clusters_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_initial_labels.csv\")\n\n    cluster_id_columns: list[str] = _filter_id_columns(clusters_df.columns)\n    # \u30dc\u30c8\u30e0\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u30fb\u8aac\u660e\u3068\u30af\u30e9\u30b9\u30bfid\u4ed8\u304d\u306e\u5404argument\u3092\u5165\u529b\u3057\u3001\u5404\u968e\u5c64\u306e\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\u30fb\u8aac\u660e\u3092\u751f\u6210\u3057\u3001argument\u306b\u4ed8\u3051\u305fdf\u3092\u4f5c\u6210\n    merge_result_df = merge_labelling(\n        clusters_df=clusters_df,\n        cluster_id_columns=sorted(cluster_id_columns, reverse=True),\n        config=config,\n    )\n    # \u4e0a\u8a18\u306edf\u304b\u3089\u5404\u30af\u30e9\u30b9\u30bf\u306elevel, id, label, description, value\u3092\u53d6\u5f97\u3057\u3066df\u3092\u4f5c\u6210\n    melted_df = melt_cluster_data(merge_result_df)\n    # \u4e0a\u8a18\u306edf\u306b\u89aa\u5b50\u95a2\u4fc2\u3092\u8ffd\u52a0\n    parent_child_df = _build_parent_child_mapping(merge_result_df, cluster_id_columns)\n    melted_df = melted_df.merge(parent_child_df, on=[\"level\", \"id\"], how=\"left\")\n    density_df = calculate_cluster_density(melted_df, config)\n    density_df.to_csv(merge_path, index=False)\n\n\ndef _build_parent_child_mapping(df: pd.DataFrame, cluster_id_columns: list[str]):\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u9593\u306e\u89aa\u5b50\u95a2\u4fc2\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3059\u308b\n\n    Args:\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        cluster_id_columns: \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n\n    Returns:\n        \u89aa\u5b50\u95a2\u4fc2\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u60c5\u5831\u3092\u542b\u3080DataFrame\n    \"\"\"\n    results = []\n    top_cluster_column = cluster_id_columns[0]\n    top_cluster_values = df[top_cluster_column].unique()\n    for c in top_cluster_values:\n        results.append(\n            {\n                \"level\": 1,\n                \"id\": c,\n                \"parent\": \"0\",  # aggregation\u3067\u8ffd\u52a0\u3059\u308b\u5168\u4f53\u30af\u30e9\u30b9\u30bf\u306eid\n            }\n        )\n\n    for idx in range(len(cluster_id_columns) - 1):\n        current_column = cluster_id_columns[idx]\n        children_column = cluster_id_columns[idx + 1]\n        current_level = current_column.replace(\"-id\", \"\").replace(\"cluster-level-\", \"\")\n        # \u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bfid\n        current_cluster_values = df[current_column].unique()\n        for current_id in current_cluster_values:\n            children_ids = df.loc[df[current_column] == current_id, children_column].unique()\n            for child_id in children_ids:\n                results.append(\n                    {\n                        \"level\": int(current_level) + 1,\n                        \"id\": child_id,\n                        \"parent\": current_id,\n                    }\n                )\n    return pd.DataFrame(results)\n\n\ndef _filter_id_columns(columns: list[str]) -> list[str]:\n    \"\"\"\u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\n\n    Args:\n        columns: \u5168\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n\n    Returns:\n        \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    return [col for col in columns if col.startswith(\"cluster-level-\") and col.endswith(\"-id\")]\n\n\ndef melt_cluster_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u30c7\u30fc\u30bf\u3092\u884c\u5f62\u5f0f\u306b\u5909\u63db\u3059\u308b\n\n    cluster-level-n-(id|label|description) \u3092\u884c\u5f62\u5f0f (level, id, label, description, value) \u306b\u307e\u3068\u3081\u308b\u3002\n    [cluster-level-n-id, cluster-level-n-label, cluster-level-n-description] \u3092 [level, id, label, description, value(\u4ef6\u6570)] \u306b\u5909\u63db\u3059\u308b\u3002\n\n    Args:\n        df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n\n    Returns:\n        \u884c\u5f62\u5f0f\u306b\u5909\u63db\u3055\u308c\u305fDataFrame\n    \"\"\"\n    id_columns: list[str] = _filter_id_columns(df.columns)\n    levels: set[int] = {int(col.replace(\"cluster-level-\", \"\").replace(\"-id\", \"\")) for col in id_columns}\n    all_rows: list[dict] = []\n\n    # level\u3054\u3068\u306b\u5404\u30af\u30e9\u30b9\u30bf\u306e\u51fa\u73fe\u4ef6\u6570\u3092\u96c6\u8a08\u30fb\u7e26\u6301\u3061\u306b\u3059\u308b\n    for level in levels:\n        cluster_columns = ClusterColumns.from_id_column(f\"cluster-level-{level}-id\")\n        # \u30af\u30e9\u30b9\u30bfid\u3054\u3068\u306e\u4ef6\u6570\u96c6\u8a08\n        level_count_df = df.groupby(cluster_columns.id).size().reset_index(name=\"value\")\n\n        level_unique_val_df = df[\n            [cluster_columns.id, cluster_columns.label, cluster_columns.description]\n        ].drop_duplicates()\n        level_unique_val_df = level_unique_val_df.merge(level_count_df, on=cluster_columns.id, how=\"left\")\n        level_unique_vals = [\n            {\n                \"level\": level,\n                \"id\": row[cluster_columns.id],\n                \"label\": row[cluster_columns.label],\n                \"description\": row[cluster_columns.description],\n                \"value\": row[\"value\"],\n            }\n            for _, row in level_unique_val_df.iterrows()\n        ]\n        all_rows.extend(level_unique_vals)\n    return pd.DataFrame(all_rows)\n\n\ndef merge_labelling(clusters_df: pd.DataFrame, cluster_id_columns: list[str], config) -> pd.DataFrame:\n    \"\"\"\u968e\u5c64\u7684\u306a\u30af\u30e9\u30b9\u30bf\u306e\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        clusters_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        cluster_id_columns: \u30af\u30e9\u30b9\u30bfID\u306e\u30ab\u30e9\u30e0\u540d\u306e\u30ea\u30b9\u30c8\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n\n    Returns:\n        \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080DataFrame\n    \"\"\"\n    for idx in tqdm(range(len(cluster_id_columns) - 1)):\n        previous_columns = ClusterColumns.from_id_column(cluster_id_columns[idx])\n        current_columns = ClusterColumns.from_id_column(cluster_id_columns[idx + 1])\n\n        process_fn = partial(\n            process_merge_labelling,\n            result_df=clusters_df,\n            current_columns=current_columns,\n            previous_columns=previous_columns,\n            config=config,\n        )\n\n        current_cluster_ids = sorted(clusters_df[current_columns.id].unique())\n        with ThreadPoolExecutor(max_workers=config[\"hierarchical_merge_labelling\"][\"workers\"]) as executor:\n            responses = list(\n                tqdm(\n                    executor.map(process_fn, current_cluster_ids),\n                    total=len(current_cluster_ids),\n                )\n            )\n\n        current_result_df = pd.DataFrame(responses)\n        clusters_df = clusters_df.merge(current_result_df, on=[current_columns.id])\n    return clusters_df\n\n\nclass LabellingFromat(BaseModel):\n    \"\"\"\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3092\u5b9a\u7fa9\u3059\u308b\"\"\"\n\n    label: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u30e9\u30d9\u30eb\u540d\")\n    description: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u306e\u8aac\u660e\u6587\")\n\n\ndef process_merge_labelling(\n    target_cluster_id: str,\n    result_df: pd.DataFrame,\n    current_columns: ClusterColumns,\n    previous_columns: ClusterColumns,\n    config,\n):\n    \"\"\"\u500b\u5225\u306e\u30af\u30e9\u30b9\u30bf\u306b\u5bfe\u3057\u3066\u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        target_cluster_id: \u51e6\u7406\u5bfe\u8c61\u306e\u30af\u30e9\u30b9\u30bfID\n        result_df: \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u7d50\u679c\u306eDataFrame\n        current_columns: \u73fe\u5728\u306e\u30ec\u30d9\u30eb\u306e\u30ab\u30e9\u30e0\u60c5\u5831\n        previous_columns: \u524d\u306e\u30ec\u30d9\u30eb\u306e\u30ab\u30e9\u30e0\u60c5\u5831\n        config: \u8a2d\u5b9a\u60c5\u5831\u3092\u542b\u3080\u8f9e\u66f8\n\n    Returns:\n        \u30de\u30fc\u30b8\u30e9\u30d9\u30ea\u30f3\u30b0\u7d50\u679c\u3092\u542b\u3080\u8f9e\u66f8\n    \"\"\"\n\n    def filter_previous_values(df: pd.DataFrame, previous_columns: ClusterColumns) -> list[ClusterValues]:\n        \"\"\"\u524d\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u60c5\u5831\u3092\u53d6\u5f97\u3059\u308b\"\"\"\n        previous_records = df[df[current_columns.id] == target_cluster_id][\n            [previous_columns.label, previous_columns.description]\n        ].drop_duplicates()\n        previous_values = [\n            ClusterValues(\n                label=row[previous_columns.label],\n                description=row[previous_columns.description],\n            )\n            for _, row in previous_records.iterrows()\n        ]\n        return previous_values\n\n    previous_values = filter_previous_values(result_df, previous_columns)\n    if len(previous_values) == 1:\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: previous_values[0].label,\n            current_columns.description: previous_values[0].description,\n        }\n    elif len(previous_values) == 0:\n        raise ValueError(f\"\u30af\u30e9\u30b9\u30bf {target_cluster_id} \u306b\u306f\u524d\u306e\u30ec\u30d9\u30eb\u306e\u30af\u30e9\u30b9\u30bf\u304c\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\")\n\n    current_cluster_data = result_df[result_df[current_columns.id] == target_cluster_id]\n    sampling_num = min(\n        config[\"hierarchical_merge_labelling\"][\"sampling_num\"],\n        len(current_cluster_data),\n    )\n    sampled_data = current_cluster_data.sample(sampling_num)\n    sampled_argument_text = \"\\n\".join(sampled_data[\"argument\"].values)\n    cluster_text = \"\\n\".join([value.to_prompt_text() for value in previous_values])\n    messages = [\n        {\"role\": \"system\", \"content\": config[\"hierarchical_merge_labelling\"][\"prompt\"]},\n        {\n            \"role\": \"user\",\n            \"content\": \"\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\\n\" + cluster_text + \"\\n\" + \"\u30af\u30e9\u30b9\u30bf\u306e\u610f\u898b\\n\" + sampled_argument_text,\n        },\n    ]\n    try:\n        response = request_to_chat_openai(\n            messages=messages,\n            model=config[\"hierarchical_merge_labelling\"][\"model\"],\n            json_schema=LabellingFromat,\n            provider=config.get(\"provider\", \"openai\"),\n            local_llm_address=config.get(\"local_llm_address\"),\n        )\n        response_json = json.loads(response) if isinstance(response, str) else response\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: response_json.get(\"label\", \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n            current_columns.description: response_json.get(\"description\", \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\"),\n        }\n    except Exception as e:\n        print(f\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f: {e}\")\n        return {\n            current_columns.id: target_cluster_id,\n            current_columns.label: \"\u30a8\u30e9\u30fc\u3067\u30e9\u30d9\u30eb\u540d\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n            current_columns.description: \"\u30a8\u30e9\u30fc\u3067\u89e3\u8aac\u304c\u53d6\u5f97\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f\",\n        }\n\n\ndef calculate_cluster_density(melted_df: pd.DataFrame, config: dict):\n    \"\"\"\u30af\u30e9\u30b9\u30bf\u5185\u306e\u5bc6\u5ea6\u8a08\u7b97\"\"\"\n    hierarchical_cluster_df = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n\n    densities = []\n    for level, c_id in zip(melted_df[\"level\"], melted_df[\"id\"], strict=False):\n        cluster_embeds = hierarchical_cluster_df[hierarchical_cluster_df[f\"cluster-level-{level}-id\"] == c_id][\n            [\"x\", \"y\"]\n        ].values\n        density = calculate_density(cluster_embeds)\n        densities.append(density)\n\n    # \u5bc6\u5ea6\u306e\u30e9\u30f3\u30af\u3092\u8a08\u7b97\n    melted_df[\"density\"] = densities\n    melted_df[\"density_rank\"] = melted_df.groupby(\"level\")[\"density\"].rank(ascending=False, method=\"first\")\n    melted_df[\"density_rank_percentile\"] = melted_df.groupby(\"level\")[\"density_rank\"].transform(lambda x: x / len(x))\n    return melted_df\n\n\ndef calculate_density(embeds: np.ndarray):\n    \"\"\"\u5e73\u5747\u8ddd\u96e2\u306b\u57fa\u3065\u3044\u3066\u5bc6\u5ea6\u3092\u8a08\u7b97\"\"\"\n    center = np.mean(embeds, axis=0)\n    distances = np.linalg.norm(embeds - center, axis=1)\n    avg_distance = np.mean(distances)\n    density = 1 / (avg_distance + 1e-10)\n    return density\n",
          "model": "gpt-4o-mini"
        }
      },
      {
        "step": "hierarchical_overview",
        "completed": "2025-05-13T07:58:01.153541",
        "duration": 2.036744,
        "params": {
          "prompt": "/system \n\n\u3042\u306a\u305f\u306f\u30b7\u30f3\u30af\u30bf\u30f3\u30af\u3067\u50cd\u304f\u5c02\u9580\u306e\u30ea\u30b5\u30fc\u30c1\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n\u30c1\u30fc\u30e0\u306f\u7279\u5b9a\u306e\u30c6\u30fc\u30de\u306b\u95a2\u3057\u3066\u30d1\u30d6\u30ea\u30c3\u30af\u30fb\u30b3\u30f3\u30b5\u30eb\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u5b9f\u65bd\u3057\u3001\u7570\u306a\u308b\u9078\u629e\u80a2\u306e\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u3092\u5206\u6790\u3057\u59cb\u3081\u3066\u3044\u307e\u3059\u3002\n\u3053\u308c\u304b\u3089\u610f\u898b\u30b0\u30eb\u30fc\u30d7\u306e\u30ea\u30b9\u30c8\u3068\u305d\u306e\u7c21\u5358\u306a\u5206\u6790\u304c\u63d0\u4f9b\u3055\u308c\u307e\u3059\u3002\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u3001\u8abf\u67fb\u7d50\u679c\u306e\u7c21\u6f54\u306a\u8981\u7d04\u3092\u8fd4\u3059\u3053\u3068\u3067\u3059\u3002\u8981\u7d04\u306f\u975e\u5e38\u306b\u7c21\u6f54\u306b\uff08\u6700\u5927\u30671\u6bb5\u843d\u3001\u6700\u59274\u6587\uff09\u307e\u3068\u3081\u3001\u7121\u610f\u5473\u306a\u8a00\u8449\u3092\u907f\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n\u51fa\u529b\u306f\u65e5\u672c\u8a9e\u3067\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n",
          "source_code": "\"\"\"Create summaries for the clusters.\"\"\"\n\nimport json\nimport re\n\nimport pandas as pd\nfrom pydantic import BaseModel, Field\n\nfrom services.llm import request_to_chat_openai\n\n\nclass OverviewResponse(BaseModel):\n    summary: str = Field(..., description=\"\u30af\u30e9\u30b9\u30bf\u30fc\u306e\u5168\u4f53\u7684\u306a\u8981\u7d04\")\n\n\ndef hierarchical_overview(config):\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/hierarchical_overview.txt\"\n\n    hierarchical_label_df = pd.read_csv(f\"outputs/{dataset}/hierarchical_merge_labels.csv\")\n\n    prompt = config[\"hierarchical_overview\"][\"prompt\"]\n    model = config[\"hierarchical_overview\"][\"model\"]\n    provider = config.get(\"provider\", \"openai\")  # \u30c7\u30d5\u30a9\u30eb\u30c8\u306fopenai\n\n    # TODO: level1\u3067\u56fa\u5b9a\u306b\u3057\u3066\u3044\u308b\u304c\u3001\u8a2d\u5b9a\u3067\u5909\u3048\u3089\u308c\u308b\u3088\u3046\u306b\u3059\u308b\n    target_level = 1\n    target_records = hierarchical_label_df[hierarchical_label_df[\"level\"] == target_level]\n    ids = target_records[\"id\"].to_list()\n    labels = target_records[\"label\"].to_list()\n    descriptions = target_records[\"description\"].to_list()\n    target_records.set_index(\"id\", inplace=True)\n\n    input_text = \"\"\n    for i, _ in enumerate(ids):\n        input_text += f\"# Cluster {i}/{len(ids)}: {labels[i]}\\n\\n\"\n        input_text += descriptions[i] + \"\\n\\n\"\n\n    messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": input_text}]\n    response = request_to_chat_openai(\n        messages=messages,\n        model=model,\n        provider=provider,\n        local_llm_address=config.get(\"local_llm_address\"),\n        json_schema=OverviewResponse,\n    )\n\n    try:\n        # structured output\u3068\u3057\u3066\u30d1\u30fc\u30b9\u3067\u304d\u308b\u306a\u3089\u51e6\u7406\u3059\u308b\n        if isinstance(response, dict):\n            parsed_response = response\n        else:\n            parsed_response = json.loads(response)\n\n        with open(path, \"w\") as file:\n            file.write(parsed_response[\"summary\"])\n\n    except Exception:\n        # think\u30bf\u30b0\u304c\u51fa\u529b\u3055\u308c\u308bReasoning\u30e2\u30c7\u30eb\u7528\u306b\u3001think\u30bf\u30b0\u3092\u9664\u53bb\u3059\u308b\n        thinking_removed = re.sub(\n            r\"<think\\b[^>]*>.*?</think>\",\n            \"\",\n            response,\n            flags=re.DOTALL,\n        )\n\n        with open(path, \"w\") as file:\n            file.write(thinking_removed)\n",
          "model": "gpt-4o-mini"
        }
      },
      {
        "step": "hierarchical_aggregation",
        "completed": "2025-05-13T07:58:01.211746",
        "duration": 0.055275,
        "params": {
          "sampling_num": 30,
          "hidden_properties": {},
          "source_code": "\"\"\"Generate a convenient JSON output file.\"\"\"\n\nimport json\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import TypedDict\n\nimport pandas as pd\n\nROOT_DIR = Path(__file__).parent.parent.parent.parent\nCONFIG_DIR = ROOT_DIR / \"scatter\" / \"pipeline\" / \"configs\"\n\n\nclass Argument(TypedDict):\n    arg_id: str\n    argument: str\n    comment_id: str\n    x: float\n    y: float\n    p: float\n    cluster_ids: list[str]\n\n\nclass Cluster(TypedDict):\n    level: int\n    id: str\n    label: str\n    takeaway: str\n    value: int\n    parent: str\n    density_rank_percentile: float | None\n\n\ndef hierarchical_aggregation(config):\n    path = f\"outputs/{config['output_dir']}/hierarchical_result.json\"\n    results = {\n        \"arguments\": [],\n        \"clusters\": [],\n        \"comments\": {},\n        \"propertyMap\": {},\n        \"translations\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index(\"arg-id\", inplace=True)\n    arg_num = len(arguments)\n    relation_df = pd.read_csv(f\"outputs/{config['output_dir']}/relations.csv\")\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/hierarchical_merge_labels.csv\")\n\n    hidden_properties_map: dict[str, list[str]] = config[\"hierarchical_aggregation\"][\"hidden_properties\"]\n\n    results[\"arguments\"] = _build_arguments(clusters)\n    results[\"clusters\"] = _build_cluster_value(labels, arg_num)\n    # NOTE: \u5c5e\u6027\u306b\u5fdc\u3058\u305f\u30b3\u30e1\u30f3\u30c8\u30d5\u30a3\u30eb\u30bf\u6a5f\u80fd\u304c\u5b9f\u88c5\u3055\u308c\u3066\u304a\u3089\u305a\u3001\u5168\u3066\u306e\u30b3\u30e1\u30f3\u30c8\u304c\u542b\u307e\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3001\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\n    # results[\"comments\"] = _build_comments_value(\n    #     comments, arguments, hidden_properties_map\n    # )\n    results[\"comment_num\"] = len(comments)\n    results[\"translations\"] = _build_translations(config)\n    # \u5c5e\u6027\u60c5\u5831\u306e\u30ab\u30e9\u30e0\u306f\u3001\u5143\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u6307\u5b9a\u3057\u305f\u30ab\u30e9\u30e0\u3068classification\u3059\u308b\u30ab\u30c6\u30b4\u30ea\u3092\u5408\u308f\u305b\u305f\u3082\u306e\n    results[\"propertyMap\"] = _build_property_map(arguments, hidden_properties_map, config)\n\n    with open(f\"outputs/{config['output_dir']}/hierarchical_overview.txt\") as f:\n        overview = f.read()\n    print(\"overview\")\n    print(overview)\n    results[\"overview\"] = overview\n\n    with open(path, \"w\") as file:\n        json.dump(results, file, indent=2, ensure_ascii=False)\n    # TODO: \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30ed\u30b8\u30c3\u30af\u3092\u5b9f\u88c5\u3057\u305f\u3044\u304c\u3001\u73fe\u72b6\u306f\u5168\u4ef6\u62bd\u51fa\n    create_custom_intro(config)\n    if config[\"is_pubcom\"]:\n        add_original_comments(labels, arguments, relation_df, clusters, config)\n\n\ndef create_custom_intro(config):\n    dataset = config[\"output_dir\"]\n    args_path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    result_path = f\"outputs/{dataset}/hierarchical_result.json\"\n\n    input_count = len(comments)\n    args_count = len(pd.read_csv(args_path))\n    processed_num = min(input_count, config[\"extraction\"][\"limit\"])\n\n    print(f\"Input count: {input_count}\")\n    print(f\"Args count: {args_count}\")\n\n    base_custom_intro = \"\"\"{intro}\n\u5206\u6790\u5bfe\u8c61\u3068\u306a\u3063\u305f\u30c7\u30fc\u30bf\u306e\u4ef6\u6570\u306f{processed_num}\u4ef6\u3067\u3001\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066OpenAI API\u3092\u7528\u3044\u3066{args_count}\u4ef6\u306e\u610f\u898b\uff08\u8b70\u8ad6\uff09\u3092\u62bd\u51fa\u3057\u3001\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u884c\u3063\u305f\u3002\n\"\"\"\n\n    intro = config[\"intro\"]\n    custom_intro = base_custom_intro.format(intro=intro, processed_num=processed_num, args_count=args_count)\n\n    with open(result_path) as f:\n        result = json.load(f)\n    result[\"config\"][\"intro\"] = custom_intro\n    with open(result_path, \"w\") as f:\n        json.dump(result, f, indent=2, ensure_ascii=False)\n\n\ndef add_original_comments(labels, arguments, relation_df, clusters, config):\n    # \u5927\u30ab\u30c6\u30b4\u30ea\uff08cluster-level-1\uff09\u306b\u8a72\u5f53\u3059\u308b\u30e9\u30d9\u30eb\u3060\u3051\u62bd\u51fa\n    labels_lv1 = labels[labels[\"level\"] == 1][[\"id\", \"label\"]].rename(\n        columns={\"id\": \"cluster-level-1-id\", \"label\": \"category_label\"}\n    )\n\n    # arguments \u3068 clusters \u3092\u30de\u30fc\u30b8\uff08\u30ab\u30c6\u30b4\u30ea\u60c5\u5831\u4ed8\u4e0e\uff09\n    merged = arguments.merge(clusters[[\"arg-id\", \"cluster-level-1-id\"]], on=\"arg-id\").merge(\n        labels_lv1, on=\"cluster-level-1-id\", how=\"left\"\n    )\n\n    # relation_df \u3068\u7d50\u5408\n    merged = merged.merge(relation_df, on=\"arg-id\", how=\"left\")\n\n    # \u5143\u30b3\u30e1\u30f3\u30c8\u53d6\u5f97\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    comments[\"comment-id\"] = comments[\"comment-id\"].astype(str)\n    merged[\"comment-id\"] = merged[\"comment-id\"].astype(str)\n\n    # \u5143\u30b3\u30e1\u30f3\u30c8\u672c\u6587\u306a\u3069\u3068\u30de\u30fc\u30b8\n    final_df = merged.merge(comments, on=\"comment-id\", how=\"left\")\n\n    # \u5fc5\u8981\u30ab\u30e9\u30e0\u306e\u307f\u6574\u5f62\n    final_cols = [\"comment-id\", \"comment-body\", \"arg-id\", \"argument\", \"cluster-level-1-id\", \"category_label\"]\n    for col in [\"source\", \"url\"]:\n        if col in comments.columns:\n            final_cols.append(col)\n\n    final_df = final_df[final_cols]\n    final_df = final_df.rename(\n        columns={\n            \"cluster-level-1-id\": \"category_id\",\n            \"category_label\": \"category\",\n            \"arg-id\": \"arg_id\",\n            \"argument\": \"argument\",\n            \"comment-body\": \"original-comment\",\n        }\n    )\n\n    # \u4fdd\u5b58\n    final_df.to_csv(f\"outputs/{config['output_dir']}/final_result_with_comments.csv\", index=False)\n\n\ndef _build_arguments(clusters: pd.DataFrame) -> list[Argument]:\n    cluster_columns = [col for col in clusters.columns if col.startswith(\"cluster-level-\") and \"id\" in col]\n\n    arguments: list[Argument] = []\n    for _, row in clusters.iterrows():\n        cluster_ids = [\"0\"]\n        for cluster_column in cluster_columns:\n            cluster_ids.append(row[cluster_column])\n        argument: Argument = {\n            \"arg_id\": row[\"arg-id\"],\n            \"argument\": row[\"argument\"],\n            \"x\": row[\"x\"],\n            \"y\": row[\"y\"],\n            \"p\": 0,  # NOTE: \u4e00\u65e6\u5168\u90e80\u3067\u3044\u308c\u308b\n            \"cluster_ids\": cluster_ids,\n        }\n        arguments.append(argument)\n    return arguments\n\n\ndef _build_cluster_value(melted_labels: pd.DataFrame, total_num: int) -> list[Cluster]:\n    results: list[Cluster] = [\n        Cluster(\n            level=0,\n            id=\"0\",\n            label=\"\u5168\u4f53\",\n            takeaway=\"\",\n            value=total_num,\n            parent=\"\",\n            density_rank_percentile=0,\n        )\n    ]\n\n    for _, melted_label in melted_labels.iterrows():\n        cluster_value = Cluster(\n            level=melted_label[\"level\"],\n            id=melted_label[\"id\"],\n            label=melted_label[\"label\"],\n            takeaway=melted_label[\"description\"],\n            value=melted_label[\"value\"],\n            parent=melted_label.get(\"parent\", \"\u5168\u4f53\"),\n            density_rank_percentile=melted_label.get(\"density_rank_percentile\"),\n        )\n        results.append(cluster_value)\n    return results\n\n\ndef _build_comments_value(\n    comments: pd.DataFrame,\n    arguments: pd.DataFrame,\n    hidden_properties_map: dict[str, list[str]],\n):\n    comment_dict: dict[str, dict[str, str]] = {}\n    useful_comment_ids = set(arguments[\"comment-id\"].values)\n    for _, row in comments.iterrows():\n        id = row[\"comment-id\"]\n        if id in useful_comment_ids:\n            res = {\"comment\": row[\"comment-body\"]}\n            should_skip = any(row[prop] in hidden_values for prop, hidden_values in hidden_properties_map.items())\n            if should_skip:\n                continue\n            comment_dict[str(id)] = res\n\n    return comment_dict\n\n\ndef _build_translations(config):\n    languages = list(config.get(\"translation\", {}).get(\"languages\", []))\n    if len(languages) > 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        return json.loads(translations)\n    return {}\n\n\ndef _build_property_map(\n    arguments: pd.DataFrame, hidden_properties_map: dict[str, list[str]], config: dict\n) -> dict[str, dict[str, str]]:\n    property_columns = list(hidden_properties_map.keys()) + list(config[\"extraction\"][\"categories\"].keys())\n    property_map = defaultdict(dict)\n\n    # \u6307\u5b9a\u3055\u308c\u305f property_columns \u304c arguments \u306b\u5b58\u5728\u3059\u308b\u304b\u30c1\u30a7\u30c3\u30af\n    missing_cols = [col for col in property_columns if col not in arguments.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"\u6307\u5b9a\u3055\u308c\u305f\u30ab\u30e9\u30e0 {missing_cols} \u304c args.csv \u306b\u5b58\u5728\u3057\u307e\u305b\u3093\u3002\"\n            \"\u8a2d\u5b9a\u30d5\u30a1\u30a4\u30ebaggregation / hidden_properties\u304b\u3089\u8a72\u5f53\u30ab\u30e9\u30e0\u3092\u53d6\u308a\u9664\u3044\u3066\u304f\u3060\u3055\u3044\u3002\"\n        )\n\n    for prop in property_columns:\n        for arg_id, row in arguments.iterrows():\n            # LLM\u306b\u3088\u308bcategory classification\u304c\u3046\u307e\u304f\u884c\u304b\u305a\u3001NaN\u306e\u5834\u5408\u306fNone\u306b\u3059\u308b\n            property_map[prop][arg_id] = row[prop] if not pd.isna(row[prop]) else None\n    return property_map\n"
        }
      }
    ],
    "lock_until": "2025-05-13T08:03:01.215007",
    "current_job": "hierarchical_aggregation",
    "current_job_started": "2025-05-13T07:58:01.156476",
    "end_time": "2025-05-13T07:58:01.215000"
  },
  "embedding": {
    "model": "text-embedding-3-small",
    "source_code": "import pandas as pd\nfrom tqdm import tqdm\n\nfrom services.llm import request_to_embed\n\n\ndef embedding(config):\n    model = config[\"embedding\"][\"model\"]\n    is_embedded_at_local = config[\"is_embedded_at_local\"]\n    # print(\"start embedding\")\n    # print(f\"embedding model: {model}, is_embedded_at_local: {is_embedded_at_local}\")\n\n    dataset = config[\"output_dir\"]\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\", usecols=[\"arg-id\", \"argument\"])\n    embeddings = []\n    batch_size = 1000\n    for i in tqdm(range(0, len(arguments), batch_size)):\n        args = arguments[\"argument\"].tolist()[i : i + batch_size]\n        embeds = request_to_embed(args, model, is_embedded_at_local, config[\"provider\"])\n        embeddings.extend(embeds)\n    df = pd.DataFrame([{\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e} for i, e in enumerate(embeddings)])\n    df.to_pickle(path)\n"
  },
  "hierarchical_visualization": {
    "replacements": [],
    "source_code": "import subprocess\n\n\ndef hierarchical_visualization(config):\n    output_dir = config[\"output_dir\"]\n    cwd = \"../report\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=cwd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == \"\" and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"
  },
  "plan": [
    {
      "step": "extraction",
      "run": true,
      "reason": "forced with -f"
    },
    {
      "step": "embedding",
      "run": true,
      "reason": "forced with -f"
    },
    {
      "step": "hierarchical_clustering",
      "run": true,
      "reason": "forced with -f"
    },
    {
      "step": "hierarchical_initial_labelling",
      "run": true,
      "reason": "forced with -f"
    },
    {
      "step": "hierarchical_merge_labelling",
      "run": true,
      "reason": "forced with -f"
    },
    {
      "step": "hierarchical_overview",
      "run": true,
      "reason": "forced with -f"
    },
    {
      "step": "hierarchical_aggregation",
      "run": true,
      "reason": "forced with -f"
    },
    {
      "step": "hierarchical_visualization",
      "run": false,
      "reason": "skipping html output"
    }
  ],
  "status": "running",
  "start_time": "2025-05-16T17:08:37.349831",
  "completed_jobs": [],
  "lock_until": "2025-05-16T17:13:37.349840"
}